{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYC3OYfkgvSWr43Mf4BPLT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Punto 1: \"Análisis Avanzado con Regresión Lineal Múltiple en Python: Explorando el Conjunto de Datos de Calidad del Aire\"**\n","\n","\n","https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set\n","\n","### Objetivo\n","- Desarrollar habilidades en la implementación y análisis de modelos de regresión lineal múltiple utilizando Python en Jupyter Notebook.\n","- Comprender y aplicar técnicas de análisis de residuos, selección de variables y pruebas de bondad de ajuste.\n","- Realizar predicciones y análisis comparativos entre varios modelos.\n","\n","### Conjunto de Datos\n","- **Dataset**:Contiene las respuestas de un dispositivo multisensor de gases desplegado sobre el terreno en una ciudad italiana. Se registran los promedios de las respuestas horarias junto con las referencias de las concentraciones de gas de un análisis certificado.\n","\n","\n","### Estructura del Taller\n","\n","1. **Introducción y Preparación del Entorno**:\n","   - Configuración del entorno de Jupyter Notebook.\n","   - Importación de librerías necesarias (pandas, numpy, matplotlib, seaborn, scikit-learn, statsmodels).\n","\n","2. **Exploración y Preprocesamiento de Datos**:\n","   - Carga y exploración inicial del conjunto de datos.\n","   - Limpieza de datos y manejo de valores faltantes.\n","   - Visualización de distribuciones y correlaciones u otros.\n","\n","3. **Construcción de Modelos de Regresión Lineal Múltiple**:\n","   - Creación de modelos utilizando diferentes combinaciones de variables.\n","   - Uso de técnicas de selección de variables (como el método de eliminación hacia atrás).\n","\n","4. **Análisis de Residuos y Diagnóstico de Modelos**:\n","   - Evaluación de la normalidad de los residuos (prueba de Shapiro-Wilk).\n","   - Análisis de gráficos de residuos, multicolinealidad y datos influyentes.\n","   - Proponer correcciones necesarrias: Selección de variables, transformaciones u otra que parta de sus analisis.\n","   - Reajustar los modelos resultantes.\n","\n","5. **Comparación entre Modelos**:\n","   - Uso de métricas (R-cuadrado, RMSE, etc.) para comparar modelos.\n","   - Análisis de la significancia de las variables (p-valores, intervalos de confianza).\n","\n","6. **Pruebas de Bondad de Ajuste**:\n","  - Interpretación de resultados para evaluar la calidad del ajuste de los distintos modelos.\n","   - Realizar pruebas como ANOVA para comparar modelos (recuerde que deben ser anidados).\n","   \n","7. **Predicción**:\n","   - Realizar predicciones con los modelos construidos.\n","\n","8. **Documentación y Comentarios**:\n","   - Cada estudiante debe documentar su código y análisis.\n","   - Es crucial que cada salida de código tenga un análisis o explicación.\n","\n","### Entrega y Evaluación\n","- **Formato**: Notebook de Jupyter completo con código, visualizaciones y comentarios.\n","- **Criterios de Evaluación**: Correctitud técnica, profundidad del análisis, claridad en la documentación y creatividad en la exploración de datos y modelos.\n","\n"],"metadata":{"id":"klDaiQU7cUKB"}},{"cell_type":"markdown","source":["**Punto 2: \"Comparación y Optimización de Modelos de Regresión Lineal Regularizados en Python: Análisis del Conjunto de Datos de Precios de Casas de Ames\"**\n","\n","### Objetivo\n","- Explorar y comparar diferentes modelos de regresión lineal regularizados (Ridge, Lasso y ElasticNet).\n","- Utilizar la validación cruzada para optimizar los parámetros de cada modelo.\n","- Aplicar técnicas avanzadas de validación cruzada para evaluar el rendimiento de los modelos.\n","\n","### Conjunto de Datos\n","- **Dataset**: Conjunto de Datos de Precios de Casas de Ames, Iowa.\n","- **Descripción**: Incluye información sobre casi 3,000 propiedades vendidas en Ames, con 79 variables explicativas relacionadas con aspectos de las casas.\n","\n","https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n","\n","### Estructura del Taller\n","\n","1. **Preparación del Entorno y Carga de Datos**:\n","   - Configuración de Jupyter Notebook e importación de librerías (pandas, numpy, matplotlib, seaborn, sklearn).\n","   - Carga del conjunto de datos de Ames.\n","\n","2. **Preprocesamiento y Exploración de Datos**:\n","   - Limpieza de datos, manejo de valores faltantes y variables categóricas.\n","   - Análisis exploratorio y visualización de las características de las casas.\n","\n","3. **Modelos de Regresión Lineal Regularizados**:\n","   - Normalización o estandarización de características si es necesario.\n","   - Creación de modelos Ridge, Lasso y ElasticNet.\n","\n","4. **Optimización de Parámetros con Validación Cruzada**:\n","   - Usa validación cruzada (te recomiendo que aprendar en este taller a usar `GridSearchCV` de sklearn, sino usa los codigos que tenemos dentro del nuestros ejemplos) para encontrar los mejores parámetros (alpha, l1_ratio para ElasticNet).\n","\n","   Aquí un ejemplo:\n","\n","```python\n","   # Para Ridge\n","    param_grid_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}\n","    grid_ridge = GridSearchCV(Ridge(), param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\n","    grid_ridge.fit(X_train_scaled, y_train)\n","\n","    grid_ridge.best_estimator_ # Te dice el mejor alpha\n","\n","```\n","\n","5. **Comparación de Modelos y Evaluación de Rendimiento**:\n","   - Comparación de modelos basada en métricas (R-cuadrado ajustado, RMSE, etc.) usando validacion cruzada.\n","   - Análisis de los coeficientes y características más relevantes.\n","\n","6. **Validación Cruzada y Análisis de Resultados**:\n","     - Discusión sobre la selección del modelo óptimo basado en los resultados.\n","\n","\n","### Entrega y Evaluación\n","- **Formato**: Notebook de Jupyter completo con código, visualizaciones y comentarios.\n","- **Criterios de Evaluación**: Correctitud técnica, profundidad del análisis, claridad en la documentación y creatividad en la exploración de datos y modelos"],"metadata":{"id":"6-EYgR5BiBIK"}},{"cell_type":"markdown","source":["**Punto 3: \"Comparación y Optimización de Modelos de Regresión logistica\"**\n","\n","El siguiente problema fue extraído de una competencia de Kaggle con un premio de USD 5,000 dólares que fue ganado por un equipo que utilizo un modelo de regresión logística dentro de su proceso de solución.\n","\n","*Give Me Some Credit (https://www.kaggle.com/c/GiveMeSomeCredit/data): Improve on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years. Use the training data of the cs-training.csv file.*\n","\n","Estime dos modelos con (i) todas las variables disponibles y (ii) solo con el ingreso y la edad.\n","\n","1. Seleccione el mejor modelo utilizando (i) el criterio de información de Akaike y (ii) el AUC.\n","\n","2. Para cada modelo establezca el threshold en el valor que maximice el F1-score definido por:\n","\n","$$F1_{score}=2*(sensibilidad*especificidad)/(sensibilidad+especificidad)$$\n","\n","3. Construya las matrices de confusión con los threshold ya calibrados y para cada par de modelos seleccione el de mejor desempeño en el accuracy.\n","\n","4. Realice las pruebas de significancia global de los modelos con la prueba de ratio de verosimilitud. Seleccione el modelo que tenga el menor p-valor asociado. Si no es posible concluir con el p-valor seleccione el modelo con el mayor estadístico.\n","\n","5. En el modelo seleccionado, con un prueba de Wald, determine cuáles variables son significativas al 1% e interprete sus efectos en:\n","\n","\t- Log-odds\n","\t- Odds para la primera observacion\n","\t- Probabilidad para la primera observación\n","\t- Odds-Ratio\n","\t- Efectos marginales promedio\n","\n","  \n","### Entrega y Evaluación\n","- **Formato**: Notebook de Jupyter completo con código, visualizaciones y comentarios.\n","- **Criterios de Evaluación**: Correctitud técnica, profundidad del análisis, claridad en la documentación y creatividad en la exploración de datos y modelos"],"metadata":{"id":"9jIO_Vx-nyuo"}}]}