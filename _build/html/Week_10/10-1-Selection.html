

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lectura 10-1: Selección de variables y creación de modelos &#8212; MEL 202302</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Week_10/10-1-Selection';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../Syllabus.html">
  
  
  
  
  
    <p class="title logo__title">MEL 202302</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../Syllabus.html">
                    Syllabus
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_1/1-0-Schedule_week_1.html">Semana 1. Introducción al análisis estadístico</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Week_1/1-1-Introduction_pensamiento_stat.html">Lectura 1-1: Introducción al Pensamiento estadístico</a></li>




<li class="toctree-l1"><a class="reference internal" href="../Week_1/1-2-Estad%C3%ADstica_descriptiva.html">Lectura 1-2:  Estadística Descriptiva</a></li>








<li class="toctree-l1"><a class="reference internal" href="../Week_1/1-3-Visualizations.html">Lectura 1-3: Visualización de datos con Python</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_2/2-0-Schedule_week_2.html">Semana 2. Probabilidad y distribuciones Probabilidad y Distribuciones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Week_2/2-1-Prob.html">Lectura 2-1: Probabilidad y estadística</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_3/3-1-Inference.html">Lectura 3-1: Inferencia estadística</a></li>

<li class="toctree-l1"><a class="reference internal" href="../Week_3/3-2-Hypothesis.html">Lectura 3-2: Pruebas de Hipotesis</a></li>







<li class="toctree-l1"><a class="reference internal" href="../Week_3/3-3-Hypothesis_2.html">Lectura 3-3: Pruebas de Hipotesis 2.</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_4/4-1%20Correlaciones.html">Lectura 4-1: Correlaciones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Week_4/4-2-Simple_linear_regression.html">Lectura 4-2: Regresión lineal simple</a></li>




<li class="toctree-l1"><a class="reference internal" href="../Week_4/4-3-SLR_supuestos.html">Lectura 4-3: Supuestos de la regresión lineal simple</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_5/5-2-Hypothesis_Confidence.html">Lectura 5-1: Inferencia en RLS</a></li>









</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_7/7-1-Multiple_linear_regresion.html">Lectura 7-1: Regresión Lineal Múltiple</a></li>




<li class="toctree-l1"><a class="reference internal" href="../Week_7/7-2-Categorical_variables.html">Lectura 7-2: Predictores categóricos e interacciones</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Semana 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week_8/8-1-diagnostics.html">Lectura 8-1: Diagnóstico de modelos</a></li>




<li class="toctree-l1"><a class="reference internal" href="../Week_8/8-2-transformations.html">Lectura 8-2: Transformaciones</a></li>




<li class="toctree-l1"><a class="reference internal" href="../Week_8/8-3-collinearity.html">Lectura 8-3: Colinealidad</a></li>


</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/tabarespozos/MEL_202302/blob/MEL_202302/Week_10/10-1-Selection.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tabarespozos/MEL_202302" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tabarespozos/MEL_202302/issues/new?title=Issue%20on%20page%20%2FWeek_10/10-1-Selection.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Week_10/10-1-Selection.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lectura 10-1:  Selección de variables y creación de modelos</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Lectura 10-1:  Selección de variables y creación de modelos</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#criterios-de-calidad">Criterios de calidad</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#criterio-de-informacion-de-akaike">Criterio de información de Akaike</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#criterio-de-informacion-bayesiano">Criterio de información bayesiano</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-cuadrado-ajustado">R-cuadrado ajustado</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rmse-de-validacion-cruzada">RMSE de validación cruzada</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#procedimientos-de-seleccion">Procedimientos de selección</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-hacia-atras">Búsqueda hacia atrás</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-hacia-delante">Búsqueda hacia delante</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-por-pasos">Búsqueda por pasos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-exhaustiva">Búsqueda exhaustiva</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#terminos-de-orden-superior">Términos de orden superior</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#explicacion-frente-a-prediccion">Explicación frente a predicción</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explicacion">Explicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion">Predicción</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="lectura-10-1-seleccion-de-variables-y-creacion-de-modelos">
<h1>Lectura 10-1:  Selección de variables y creación de modelos<a class="headerlink" href="#lectura-10-1-seleccion-de-variables-y-creacion-de-modelos" title="Permalink to this heading">#</a></h1>
<p>En la lectura  anterior vimos cómo la correlación entre variables predictoras puede tener efectos indeseables en los modelos. Utilizamos factores de inflación de la varianza para evaluar la gravedad de los problemas de colinealidad causados por estas correlaciones. También vimos cómo el ajuste de un modelo más pequeño, omitiendo algunos de los predictores correlacionados, da como resultado un modelo que ya no sufre problemas de colinealidad. Pero, ¿cómo elegir este modelo más pequeño?</p>
<p>En esta lectura discutiremos varios <em>criterios</em> y <em>procedimientos</em> para elegir un “buen” modelo entre muchos.</p>
</section>
<section id="criterios-de-calidad">
<h1>Criterios de calidad<a class="headerlink" href="#criterios-de-calidad" title="Permalink to this heading">#</a></h1>
<p>Hasta ahora, hemos visto criterios como <span class="math notranslate nohighlight">\(R^2\)</span> y <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> para evaluar la calidad del ajuste. Sin embargo, ambos tienen un defecto fatal. Al aumentar el tamaño de un modelo, es decir, añadiendo predictores, que en el peor de los casos no puede mejorar. Es imposible añadir un predictor a un modelo y hacer <span class="math notranslate nohighlight">\(R^2\)</span> o <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> peor. Eso significa que, si tuviéramos que utilizar cualquiera de estos para elegir entre los modelos, que * siempre* simplemente elegir el modelo más grande. Con el tiempo, estaríamos simplemente ajustando al ruido.</p>
<p>Esto indica que necesitamos un criterio de calidad que tenga en cuenta el tamaño del modelo, ya que preferimos modelos pequeños que se ajusten bien. Estamos dispuestos a sacrificar una pequeña cantidad de “bondad de ajuste” para obtener un modelo más pequeño. (Aquí utilizamos “bondad de ajuste” para referirnos simplemente a la distancia que separa los datos del modelo; cuanto más pequeños sean los errores, mejor. A menudo, en estadística, la bondad de ajuste puede tener un significado más preciso). Examinaremos tres criterios que hacen esto explícitamente: <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, <span class="math notranslate nohighlight">\(\text{BIC}\)</span> y <span class="math notranslate nohighlight">\(R^2\)</span> ajustado. También estudiaremos uno, el <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> validado cruzadamente, que considera implícitamente el tamaño del modelo.</p>
<section id="criterio-de-informacion-de-akaike">
<h2>Criterio de información de Akaike<a class="headerlink" href="#criterio-de-informacion-de-akaike" title="Permalink to this heading">#</a></h2>
<p>El primer criterio que discutiremos es el Criterio de Información de Akaike, o <span class="math notranslate nohighlight">\(\text{AIC}\)</span> para abreviar. (Tenga en cuenta que, cuando <em>Akaike</em> introdujo por primera vez esta métrica, se llamaba simplemente <em>Criterio de Información de Akaike</em>. La <em>A</em> ha cambiado de significado a lo largo de los años).</p>
<p>Recordemos que la log-verosimilitud maximizada de un modelo de regresión puede ser escrito como</p>
<div class="math notranslate nohighlight">
\[\log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log\left(\frac{\text{RSS}}{n}\right) - \frac{n}{2},\]</div>
<p>donde <span class="math notranslate nohighlight">\(\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i) ^ 2\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> y <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> se eligieron para maximizar la probabilidad.</p>
<p>Entonces podemos definir <span class="math notranslate nohighlight">\(\text{AIC}\)</span> como</p>
<div class="math notranslate nohighlight">
\[\text{AIC} = -2 \log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) + 2p = n + n \log(2\pi) + n \log\left(\frac{\text{RSS}}{n}\right) + 2p,\]</div>
<p>que es una medida de la calidad del modelo. Cuanto menor sea el <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, mejor. Para ver por qué, vamos a hablar de los dos componentes principales de <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, la <strong>probabilidad</strong> (que mide la “bondad de ajuste”) y la <strong>penalización</strong> (que es una función del tamaño del modelo).</p>
<p>La parte de probabilidad de <span class="math notranslate nohighlight">\(\text{AIC}\)</span> viene dada por</p>
<div class="math notranslate nohighlight">
\[ -2 \log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) = n + n \log(2\pi) + n \log\left(\frac{\text{RSS}}{n}\right).\]</div>
<p>En aras de la comparación de modelos, el único término aquí que va a cambiar es <span class="math notranslate nohighlight">\(n \log\left(\frac{\text{RSS}}{n}\right)\)</span>, que es una función de <span class="math notranslate nohighlight">\(\text{RSS}\)</span>. El</p>
<div class="math notranslate nohighlight">
\[ n + n \log(2\pi)\]</div>
<p>serán constantes en todos los modelos aplicados a los mismos datos. Por lo tanto, cuando un modelo se ajusta bien, es decir, tiene un <span class="math notranslate nohighlight">\(\text{RSS}\)</span> bajo, entonces este componente de probabilidad será pequeño.</p>
<p>Del mismo modo, podemos discutir el componente de penalización de <span class="math notranslate nohighlight">\(\text{AIC}\)</span> que es,</p>
<div class="math notranslate nohighlight">
\[ 2p, \]</div>
<p>donde <span class="math notranslate nohighlight">\(p\)</span> es el número de parámetros <span class="math notranslate nohighlight">\(\beta\)</span> del modelo. Llamamos a esto una penalización, porque es grande cuando <span class="math notranslate nohighlight">\(p\)</span> es grande, pero buscamos encontrar un <span class="math notranslate nohighlight">\(\text{AIC}\)</span> pequeño.</p>
<p>Por lo tanto, un buen modelo, es decir, uno con un pequeño <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, tendrá un buen equilibrio entre el ajuste bien, y el uso de un pequeño número de parámetros. Para comparar modelos</p>
<div class="math notranslate nohighlight">
\[\text{AIC} = n\log\left(\frac{\text{RSS}}{n}\right) + 2p\]</div>
<p>es una expresión suficiente, ya que <span class="math notranslate nohighlight">\(n + n \log(2\pi)\)</span> es la misma en todos los modelos para cualquier conjunto de datos en particular.</p>
</section>
<section id="criterio-de-informacion-bayesiano">
<h2>Criterio de información bayesiano<a class="headerlink" href="#criterio-de-informacion-bayesiano" title="Permalink to this heading">#</a></h2>
<p>El Criterio de Información Bayesiano, o <span class="math notranslate nohighlight">\(\text{BIC}\)</span>, es similar al <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, pero tiene una penalización mayor. El <span class="math notranslate nohighlight">\(\text{BIC}\)</span> también cuantifica la compensación entre un modelo que se ajusta bien y el número de parámetros del modelo, sin embargo, para un tamaño de muestra razonable, generalmente elige un modelo más pequeño que el <span class="math notranslate nohighlight">\(\text{AIC}\)</span>. De nuevo, para la selección del modelo, utilice el
modelo con el menor <span class="math notranslate nohighlight">\(\text{BIC}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\text{BIC} = -2 \log L(\boldsymbol{\hat{\beta}}, \hat{\sigma}^2) + \log(n) p = n + n\log(2\pi) + n\log\left(\frac{\text{RSS}}{n}\right) + \log(n)p.\]</div>
<p>Nótese que la penalización <span class="math notranslate nohighlight">\(\text{AIC}\)</span> fue</p>
<div class="math notranslate nohighlight">
\[ 2p, \]</div>
<p>mientras que para <span class="math notranslate nohighlight">\(\text{BIC}\)</span>, la penalización es</p>
<div class="math notranslate nohighlight">
\[\log(n) p.\]</div>
<p>Así, para cualquier conjunto de datos donde <span class="math notranslate nohighlight">\(log(n) &gt; 2\)</span> la penalización <span class="math notranslate nohighlight">\(\text{BIC}\)</span> será mayor que la penalización <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, por lo que <span class="math notranslate nohighlight">\(\text{BIC}\)</span> probablemente preferirá un modelo más pequeño.</p>
<p>Tenga en cuenta que a veces la pena se considera una expresión general de la forma</p>
<p><span class="math notranslate nohighlight">\(k \cdot p.\)</span>$</p>
<p>Entonces, para <span class="math notranslate nohighlight">\(\text{AIC}\)</span> <span class="math notranslate nohighlight">\(k = 2\)</span>, y para <span class="math notranslate nohighlight">\(\text{BIC}\)</span> <span class="math notranslate nohighlight">\(k = \log(n)\)</span>.</p>
<p>Para comparar modelos</p>
<div class="math notranslate nohighlight">
\[\text{BIC} = n\log\left(\frac{\text{RSS}}{n}\right) + \log(n)p\]</div>
<p>es de nuevo una expresión suficiente, ya que <span class="math notranslate nohighlight">\(n + n \log(2\pi)\)</span> es el mismo en todos los modelos para cualquier conjunto de datos en particular.</p>
</section>
<section id="r-cuadrado-ajustado">
<h2>R-cuadrado ajustado<a class="headerlink" href="#r-cuadrado-ajustado" title="Permalink to this heading">#</a></h2>
<p>Recordemos,</p>
<div class="math notranslate nohighlight">
\[ R^2 = 1 - \frac{\text{SSE}}{\text{SST}} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}.\]</div>
<p>Ahora definimos</p>
<div class="math notranslate nohighlight">
\[R_a^2 = 1 - \frac{\text{SSE}/(n-p)}{\text{SST}/(n-1)} = 1 - \left(  \frac{n-1}{n-p} \right)(1-R^2)\]</div>
<p>que denominamos <span class="math notranslate nohighlight">\(R^2\)</span> ajustado.</p>
<p>A diferencia de <span class="math notranslate nohighlight">\(R^2\)</span>, que nunca puede reducirse al añadir predictores, <span class="math notranslate nohighlight">\(R^2\)</span> ajustado penaliza de forma efectiva los predictores adicionales y puede reducirse al añadir predictores. Al igual que <span class="math notranslate nohighlight">\(R^2\)</span>, cuanto mayor sea, mejor.</p>
</section>
<section id="rmse-de-validacion-cruzada">
<h2>RMSE de validación cruzada<a class="headerlink" href="#rmse-de-validacion-cruzada" title="Permalink to this heading">#</a></h2>
<p>Cada una de las tres métricas anteriores utiliza explícitamente <span class="math notranslate nohighlight">\(p\)</span>, el número de parámetros, en sus cálculos. Por lo tanto, todas ellas limitan explícitamente el tamaño de los modelos elegidos cuando se utilizan para comparar modelos.</p>
<p>Ahora introduciremos brevemente el <strong>sobreajuste</strong> y la <strong>validación cruzada</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">make_poly_data</span><span class="p">(</span><span class="n">sample_size</span><span class="o">=</span><span class="mi">11</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poly_data</span> <span class="o">=</span> <span class="n">make_poly_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Aquí hemos generado datos donde la media de <span class="math notranslate nohighlight">\(Y\)</span> es una función cuadrática de un único predictor <span class="math notranslate nohighlight">\(x\)</span>, concretamente,</p>
<div class="math notranslate nohighlight">
\[Y = 3 + x + 4 x ^ 2 + \epsilon.\]</div>
<p>Ahora ajustaremos dos modelos a estos datos, uno que tiene la forma correcta, cuadrática, y otro que es grande, que incluye términos de hasta octavo grado inclusive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit_quad model</span>
<span class="n">X_quad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">poly_data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">poly_data</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">X_quad</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_quad</span><span class="p">)</span>
<span class="n">fit_quad</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">poly_data</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">X_quad</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit_big model</span>
<span class="n">X_big</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">poly_data</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)])</span>
<span class="n">X_big</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_big</span><span class="p">)</span>
<span class="n">fit_big</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">poly_data</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">X_big</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>A continuación, representamos gráficamente los datos y los resultados de los dos modelos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">poly_data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">poly_data</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">xplot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">X_quad_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">xplot</span><span class="p">,</span> <span class="n">xplot</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">X_quad_plot</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_quad_plot</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xplot</span><span class="p">,</span> <span class="n">fit_quad</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_quad_plot</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;dodgerblue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X_big_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">xplot</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)])</span>
<span class="n">X_big_plot</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_big_plot</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xplot</span><span class="p">,</span> <span class="n">fit_big</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_big_plot</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/40deb6ead8f9c6b357eb12decb050b7993ada7d75c9e75ccc7a4191d2819b070.png" src="../_images/40deb6ead8f9c6b357eb12decb050b7993ada7d75c9e75ccc7a4191d2819b070.png" />
</div>
</div>
<p>Podemos ver que la curva azul sólida modela estos datos bastante bien. La curva naranja discontinua se ajusta mejor a los puntos, cometiendo errores más pequeños, sin embargo es poco probable que esté modelando correctamente la verdadera relación entre <span class="math notranslate nohighlight">\(x\)</span> e <span class="math notranslate nohighlight">\(y\)</span>. Se está ajustando al ruido aleatorio. Este es un ejemplo de <strong>sobreajuste</strong>.</p>
<p>Vemos que el modelo más grande tiene efectivamente una menor <span class="math notranslate nohighlight">\(\text{RMSE}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rmse_quad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fit_quad</span><span class="o">.</span><span class="n">resid</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">rmse_quad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21.256664920063006
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rmse_big</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fit_big</span><span class="o">.</span><span class="n">resid</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rmse_big</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.2385425384641273
</pre></div>
</div>
</div>
</div>
<p>Para corregirlo, introduciremos la validación cruzada. Definimos el RMSE validado de forma cruzada como</p>
<div class="math notranslate nohighlight">
\[\text{RMSE}_{\text{LOOCV}} = \sqrt{\frac{1}{n} \sum_{i=1}^n e_{[i]}^2}.\]</div>
<p>Los <span class="math notranslate nohighlight">\(e_{[i]}\)</span> son el residuo de la observación <span class="math notranslate nohighlight">\(i\)</span>ésima, cuando esa observación <strong>no</strong> se utiliza para ajustar el modelo.</p>
<div class="math notranslate nohighlight">
\[e_{[i]} = y_{i} - \hat{y}_{[i]}\]</div>
<p>Es decir, el valor ajustado se calcula como</p>
<div class="math notranslate nohighlight">
\[ \hat{y}_{[i]} = \boldsymbol{x}_i ^ \top \hat{\beta}_{[i]}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{\beta}_{[i]}\)</span> son los coeficientes estimados cuando se elimina la <span class="math notranslate nohighlight">\(i\)</span>ésima observación del conjunto de datos.</p>
<p>En general, para realizar este cálculo, tendríamos que ajustar el modelo <span class="math notranslate nohighlight">\(n\)</span> veces, una vez con cada posible observación eliminada. Sin embargo, para la validación cruzada sin exclusión y los modelos lineales, la ecuación puede reescribirse como</p>
<div class="math notranslate nohighlight">
\[\text{RMSE}_{\text{LOOCV}} = \sqrt{\frac{1}{n}\sum_{i=1}^n \left(\frac{e_{i}}{1-h_{i}}\right)^2},\]</div>
<p>donde <span class="math notranslate nohighlight">\(h_i\)</span> son los apalancamientos y <span class="math notranslate nohighlight">\(e_i\)</span> son los residuos habituales. Esto es genial, porque ahora podemos obtener el LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> ¡ajustando sólo un modelo! En la práctica, la validación cruzada de 5 o 10 pliegues es mucho más popular. Por ejemplo, en la validación cruzada de 5 pliegues, el modelo se ajusta 5 veces, cada vez dejando fuera una quinta parte de los datos, y luego prediciendo sobre esos valores. Dejaremos el examen en profundidad de la validación cruzada para un curso de aprendizaje automático, y simplemente utilizaremos LOOCV aquí.</p>
<p>Vamos a calcular LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> para ambos modelos, a continuación, discutir <em>por qué</em> queremos hacerlo. Primero escribimos una función que calcula el LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> como se define utilizando la fórmula abreviada para modelos lineales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate LOOCV RMSE</span>
<span class="k">def</span> <span class="nf">calc_loocv_rmse</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span>  <span class="c1"># This ensures that X is aligned with the model.</span>
    <span class="n">hat_matrix</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="n">loocv_resid</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resid</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">hat_matrix</span><span class="p">))</span>
    <span class="n">loocv_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loocv_resid</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loocv_rmse</span>
</pre></div>
</div>
</div>
</div>
<p>A continuación, calcula la métrica para ambos modelos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">fit_quad</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">fit_big</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32.96049393106976
433.798844282442
</pre></div>
</div>
</div>
</div>
<p>Ahora vemos que el modelo cuadrático tiene un LOOCV mucho menor <span class="math notranslate nohighlight">\(\text{RMSE}\)</span>, por lo que preferiríamos este modelo cuadrático. Esto se debe a que el modelo grande tiene <em>severely</em> over-fit los datos. Dejando un solo punto de datos fuera y ajustando el modelo grande, el ajuste resultante es muy diferente al ajuste utilizando todos los datos. Por ejemplo, dejemos fuera el tercer punto de datos y ajustemos ambos modelos, luego grafiquemos el resultado.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Models with 3rd row removed</span>
<span class="n">poly_data_removed</span> <span class="o">=</span> <span class="n">poly_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_quad_removed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">poly_data_removed</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">poly_data_removed</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">X_quad_removed</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_quad_removed</span><span class="p">)</span>
<span class="n">fit_quad_removed</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">poly_data_removed</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">X_quad_removed</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">X_big_removed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">poly_data_removed</span><span class="o">.</span><span class="n">x</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">)])</span>
<span class="n">X_big_removed</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_big_removed</span><span class="p">)</span>
<span class="n">fit_big_removed</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">poly_data_removed</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">X_big_removed</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting models with 3rd row removed</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">poly_data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">poly_data</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xplot</span><span class="p">,</span> <span class="n">fit_quad_removed</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_quad_plot</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;dodgerblue&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xplot</span><span class="p">,</span> <span class="n">fit_big_removed</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_big_plot</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8a14300898e89c9731d92fbb490f024db38d714df1d5cf9ecb27d1348e16fa35.png" src="../_images/8a14300898e89c9731d92fbb490f024db38d714df1d5cf9ecb27d1348e16fa35.png" />
</div>
</div>
<p>Vemos que, en promedio, la línea azul continua del modelo cuadrático tiene errores similares a los anteriores. Ha cambiado muy ligeramente. Sin embargo, la línea naranja discontinua para el modelo grande, tiene un error enorme en el punto que se eliminó y es muy diferente del ajuste anterior.</p>
<p>Este es el propósito de la validación cruzada. Al evaluar cómo se ajusta el modelo a los puntos que no se utilizaron para realizar la regresión, nos hacemos una idea de lo bien que funcionará el modelo para futuras observaciones. Evalúa lo bien que funciona el modelo en general, no simplemente en los datos observados.</p>
</section>
</section>
<section id="procedimientos-de-seleccion">
<h1>Procedimientos de selección<a class="headerlink" href="#procedimientos-de-seleccion" title="Permalink to this heading">#</a></h1>
<p>Ya hemos visto una serie de criterios de calidad de los modelos, pero ahora tenemos que abordar qué modelos considerar. La selección de modelos implica tanto un criterio de calidad como un procedimiento de búsqueda.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">faraway.utils</span>
<span class="kn">import</span> <span class="nn">faraway.datasets.seatpos</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># cargar base de datos de ejemplo</span>

<span class="n">seatpos</span> <span class="o">=</span> <span class="n">faraway</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">seatpos</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="n">seatpos</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Weight</th>
      <th>HtShoes</th>
      <th>Ht</th>
      <th>Seated</th>
      <th>Arm</th>
      <th>Thigh</th>
      <th>Leg</th>
      <th>hipcenter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>46</td>
      <td>180</td>
      <td>187.2</td>
      <td>184.9</td>
      <td>95.2</td>
      <td>36.1</td>
      <td>45.3</td>
      <td>41.3</td>
      <td>-206.300</td>
    </tr>
    <tr>
      <th>1</th>
      <td>31</td>
      <td>175</td>
      <td>167.5</td>
      <td>165.5</td>
      <td>83.8</td>
      <td>32.9</td>
      <td>36.5</td>
      <td>35.9</td>
      <td>-178.210</td>
    </tr>
    <tr>
      <th>2</th>
      <td>23</td>
      <td>100</td>
      <td>153.6</td>
      <td>152.2</td>
      <td>82.9</td>
      <td>26.0</td>
      <td>36.6</td>
      <td>31.0</td>
      <td>-71.673</td>
    </tr>
    <tr>
      <th>3</th>
      <td>19</td>
      <td>185</td>
      <td>190.3</td>
      <td>187.4</td>
      <td>97.3</td>
      <td>37.4</td>
      <td>44.1</td>
      <td>41.0</td>
      <td>-257.720</td>
    </tr>
    <tr>
      <th>4</th>
      <td>23</td>
      <td>159</td>
      <td>178.0</td>
      <td>174.1</td>
      <td>93.9</td>
      <td>29.5</td>
      <td>40.1</td>
      <td>36.9</td>
      <td>-173.230</td>
    </tr>
    <tr>
      <th>5</th>
      <td>47</td>
      <td>170</td>
      <td>178.7</td>
      <td>177.0</td>
      <td>92.4</td>
      <td>36.0</td>
      <td>43.2</td>
      <td>37.4</td>
      <td>-185.150</td>
    </tr>
    <tr>
      <th>6</th>
      <td>30</td>
      <td>137</td>
      <td>165.7</td>
      <td>164.6</td>
      <td>87.7</td>
      <td>32.5</td>
      <td>35.6</td>
      <td>36.2</td>
      <td>-164.750</td>
    </tr>
    <tr>
      <th>7</th>
      <td>28</td>
      <td>192</td>
      <td>185.3</td>
      <td>182.7</td>
      <td>96.9</td>
      <td>35.8</td>
      <td>39.9</td>
      <td>43.1</td>
      <td>-270.920</td>
    </tr>
    <tr>
      <th>8</th>
      <td>23</td>
      <td>150</td>
      <td>167.6</td>
      <td>165.0</td>
      <td>91.4</td>
      <td>29.4</td>
      <td>35.5</td>
      <td>33.4</td>
      <td>-151.780</td>
    </tr>
    <tr>
      <th>9</th>
      <td>29</td>
      <td>120</td>
      <td>161.2</td>
      <td>158.7</td>
      <td>85.2</td>
      <td>26.6</td>
      <td>31.0</td>
      <td>32.8</td>
      <td>-113.880</td>
    </tr>
    <tr>
      <th>10</th>
      <td>47</td>
      <td>143</td>
      <td>171.9</td>
      <td>169.1</td>
      <td>87.8</td>
      <td>32.9</td>
      <td>39.2</td>
      <td>36.9</td>
      <td>-196.150</td>
    </tr>
    <tr>
      <th>11</th>
      <td>41</td>
      <td>107</td>
      <td>155.7</td>
      <td>152.5</td>
      <td>82.9</td>
      <td>29.6</td>
      <td>32.7</td>
      <td>31.1</td>
      <td>-125.550</td>
    </tr>
    <tr>
      <th>12</th>
      <td>51</td>
      <td>227</td>
      <td>179.8</td>
      <td>177.2</td>
      <td>91.7</td>
      <td>31.1</td>
      <td>41.4</td>
      <td>40.2</td>
      <td>-203.610</td>
    </tr>
    <tr>
      <th>13</th>
      <td>30</td>
      <td>147</td>
      <td>164.9</td>
      <td>162.7</td>
      <td>88.0</td>
      <td>27.7</td>
      <td>33.6</td>
      <td>33.8</td>
      <td>-163.220</td>
    </tr>
    <tr>
      <th>14</th>
      <td>22</td>
      <td>178</td>
      <td>177.2</td>
      <td>176.4</td>
      <td>94.1</td>
      <td>31.1</td>
      <td>41.0</td>
      <td>36.6</td>
      <td>-204.110</td>
    </tr>
    <tr>
      <th>15</th>
      <td>67</td>
      <td>166</td>
      <td>177.1</td>
      <td>175.3</td>
      <td>89.4</td>
      <td>36.7</td>
      <td>40.1</td>
      <td>39.2</td>
      <td>-186.800</td>
    </tr>
    <tr>
      <th>16</th>
      <td>25</td>
      <td>153</td>
      <td>173.4</td>
      <td>171.2</td>
      <td>85.0</td>
      <td>33.1</td>
      <td>45.2</td>
      <td>38.4</td>
      <td>-228.350</td>
    </tr>
    <tr>
      <th>17</th>
      <td>65</td>
      <td>113</td>
      <td>162.6</td>
      <td>158.7</td>
      <td>85.2</td>
      <td>31.1</td>
      <td>35.7</td>
      <td>32.5</td>
      <td>-103.850</td>
    </tr>
    <tr>
      <th>18</th>
      <td>22</td>
      <td>142</td>
      <td>167.3</td>
      <td>164.6</td>
      <td>90.4</td>
      <td>29.5</td>
      <td>36.5</td>
      <td>34.0</td>
      <td>-105.690</td>
    </tr>
    <tr>
      <th>19</th>
      <td>21</td>
      <td>130</td>
      <td>172.5</td>
      <td>170.5</td>
      <td>89.7</td>
      <td>29.9</td>
      <td>35.8</td>
      <td>35.6</td>
      <td>-137.360</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the initial model with all predictors</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">seatpos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;hipcenter&#39;</span><span class="p">))</span>
<span class="n">hipcenter_mod</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">seatpos</span><span class="p">[</span><span class="s1">&#39;hipcenter&#39;</span><span class="p">],</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.687
Model:                            OLS   Adj. R-squared:                  0.600
Method:                 Least Squares   F-statistic:                     7.940
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.31e-05
Time:                        09:57:58   Log-Likelihood:                -186.73
No. Observations:                  38   AIC:                             391.5
Df Residuals:                      29   BIC:                             406.2
Df Model:                           8                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        436.4321    166.572      2.620      0.014      95.755     777.109
Age            0.7757      0.570      1.360      0.184      -0.391       1.942
Weight         0.0263      0.331      0.080      0.937      -0.651       0.703
HtShoes       -2.6924      9.753     -0.276      0.784     -22.640      17.255
Ht             0.6013     10.130      0.059      0.953     -20.117      21.319
Seated         0.5338      3.762      0.142      0.888      -7.160       8.228
Arm           -1.3281      3.900     -0.341      0.736      -9.305       6.649
Thigh         -1.1431      2.660     -0.430      0.671      -6.583       4.297
Leg           -6.4390      4.714     -1.366      0.182     -16.080       3.202
==============================================================================
Omnibus:                        0.543   Durbin-Watson:                   1.769
Prob(Omnibus):                  0.762   Jarque-Bera (JB):                0.664
Skew:                           0.157   Prob(JB):                        0.717
Kurtosis:                       2.434   Cond. No.                     8.44e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 8.44e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
const      436.432128
Age          0.775716
Weight       0.026313
HtShoes     -2.692408
Ht           0.601345
Seated       0.533752
Arm         -1.328069
Thigh       -1.143119
Leg         -6.439046
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Volvamos a los datos <code class="docutils literal notranslate"><span class="pre">seatpos</span></code> del paquete <code class="docutils literal notranslate"><span class="pre">faraway</span></code>. Consideremos ahora sólo los modelos con términos de primer orden, es decir, sin interacciones ni polinomios. Hay <em>ocho</em> predictores en este modelo. Así que si consideramos todos los modelos posibles, que van desde el uso de 0 predictores, a los ocho predictores, hay</p>
<div class="math notranslate nohighlight">
\[\sum_{k = 0}^{p - 1} {{p - 1} \choose {k}} = 2 ^ {p - 1} = 2 ^ 8 = 256\]</div>
<p>modelos posibles.</p>
<p>Si tuviéramos 10 o más predictores, ¡ya estaríamos considerando más de 1.000 modelos! Por este motivo, a menudo buscamos entre los posibles modelos de forma inteligente, pasando por alto algunos modelos que probablemente no se consideren buenos. Consideraremos tres procedimientos de búsqueda: hacia atrás, hacia delante y por pasos.</p>
<section id="busqueda-hacia-atras">
<h2>Búsqueda hacia atrás<a class="headerlink" href="#busqueda-hacia-atras" title="Permalink to this heading">#</a></h2>
<p>Los procedimientos de selección hacia atrás comienzan con todos los posibles predictores del modelo y, a continuación, consideran cómo afectará la eliminación de un único predictor a una métrica elegida. Intentémoslo con los datos de <code class="docutils literal notranslate"><span class="pre">seatpos</span></code>.</p>
<p>No hay una función incorporada en ninguno de los paquetes para hacer este procedimiento, a continuación se presenta una función que realiza la búsqueda hacia atrás usando como criterio de calidad el <span class="math notranslate nohighlight">\(\text{AIC}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
</pre></div>
</div>
</div>
</div>
<p>En el código anterior hemos cargado un nuevo paquete, se trata de <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> que es una librería que contiene utilidades de algunos pasos de algoritmos de aprendizaje de máquina que se vuelven repetitivos y que pueden mejorar algunas de las funciones nativas de otros paquetes. Aquí lo usamos para realizar los procedimientos de selección de modelos (talvez esto sea muy fácil de hacer en otro software estadísticos como R, pero aquí la función nativa no existe como tal y tendremos que hacer un poco mas de trabajo, mi idea es proporcionales el código que es funcional en todos los sentidos y que pueden usar siempre que quieran en distintos modelos de regresión, yo les recomiendo guardar estar funciones en un archivo personal de utilidades)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>

<span class="k">class</span> <span class="nc">SMWrapper</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sm_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sm_model</span> <span class="o">=</span> <span class="n">sm_model</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sm_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sm_model_</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Clone the model to make sure the original isn&#39;t fit again</span>
            <span class="n">cloned_model</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sm_model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sm_model_</span> <span class="o">=</span> <span class="n">cloned_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sm_model_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;sm_model&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">sm_model</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">parameters</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">aic_scorer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">estimator</span><span class="o">.</span><span class="n">sm_model_</span><span class="o">.</span><span class="n">aic</span>  <span class="c1"># we multiply by -1 because SequentialFeatureSelector tries to maximize the score</span>

    <span class="k">def</span> <span class="nf">bic_scorer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">estimator</span><span class="o">.</span><span class="n">sm_model_</span><span class="o">.</span><span class="n">bic</span>  <span class="c1"># we multiply by -1 because SequentialFeatureSelector tries to maximize the score</span>


<span class="k">def</span> <span class="nf">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">sm_model</span> <span class="o">=</span> <span class="n">SMWrapper</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;aic&#39;</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">sm_model</span><span class="o">.</span><span class="n">aic_scorer</span>
    <span class="k">elif</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;bic&#39;</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">sm_model</span><span class="o">.</span><span class="n">bic_scorer</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid criterion. Choose &#39;aic&#39; or &#39;bic&#39;&quot;</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">sm_model</span><span class="p">,</span> 
              <span class="n">k_features</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> 
              <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span> 
              <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
              <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> 
              <span class="n">cv</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sfs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Asumiendo que seatpos es tu DataFrame</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">seatpos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;hipcenter&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">seatpos</span><span class="p">[</span><span class="s1">&#39;hipcenter&#39;</span><span class="p">]</span>

<span class="c1"># Select features using AIC</span>
<span class="n">sfs_result_aic</span> <span class="o">=</span> <span class="n">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Features selected based on AIC</span>
<span class="n">selected_features_aic</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs_result_aic</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected Features based on AIC: </span><span class="si">{</span><span class="n">selected_features_aic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected Features based on AIC: Index([&#39;Age&#39;, &#39;HtShoes&#39;, &#39;Leg&#39;], dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adjust a full model using only the selected features</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features_aic</span><span class="p">]</span>
<span class="n">hipcenter_mod_step_aic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_selected</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_step_aic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.681
Model:                            OLS   Adj. R-squared:                  0.653
Method:                 Least Squares   F-statistic:                     24.22
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.44e-08
Time:                        09:57:58   Log-Likelihood:                -187.05
No. Observations:                  38   AIC:                             382.1
Df Residuals:                      34   BIC:                             388.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        456.2137    102.808      4.438      0.000     247.283     665.144
Age            0.5998      0.378      1.587      0.122      -0.168       1.368
HtShoes       -2.3023      1.245     -1.849      0.073      -4.833       0.228
Leg           -6.8297      4.069     -1.678      0.102     -15.099       1.440
==============================================================================
Omnibus:                        0.164   Durbin-Watson:                   1.777
Prob(Omnibus):                  0.921   Jarque-Bera (JB):                0.370
Skew:                           0.073   Prob(JB):                        0.831
Kurtosis:                       2.539   Cond. No.                     3.23e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.23e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract AIC and calculate BIC manually</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">nobs</span>  <span class="c1"># Number of observations</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">df_model</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Number of parameters, including the intercept</span>
<span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">llf</span>  <span class="c1"># Log-likelihood of the model</span>

<span class="n">aic</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_likelihood</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span>
<span class="n">bic</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">log_likelihood</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Calculated AIC:&#39;</span><span class="p">,</span> <span class="n">aic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model AIC:&#39;</span><span class="p">,</span> <span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">aic</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Calculated BIC:&#39;</span><span class="p">,</span> <span class="n">bic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model BIC:&#39;</span><span class="p">,</span> <span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">bic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Calculated AIC: 391.4633508941793
Model AIC: 391.4633508941793
Calculated BIC: 406.2016263317168
Model BIC: 406.2016263317168
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod_step_aic</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>const      456.213654
Age          0.599833
HtShoes     -2.302255
Leg         -6.829746
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>También podríamos buscar entre los posibles modelos de forma inversa utilizando <span class="math notranslate nohighlight">\(\text{BIC}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select features using BIC</span>
<span class="n">sfs_result_bic</span> <span class="o">=</span> <span class="n">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Features selected based on BIC</span>
<span class="n">selected_features_bic</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs_result_bic</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected Features based on BIC: </span><span class="si">{</span><span class="n">selected_features_bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Adjust a full model using only the selected features</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features_bic</span><span class="p">]</span>
<span class="n">hipcenter_mod_step_bic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_selected</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_step_bic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected Features based on BIC: Index([&#39;HtShoes&#39;], dtype=&#39;object&#39;)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.635
Model:                            OLS   Adj. R-squared:                  0.624
Method:                 Least Squares   F-statistic:                     62.51
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           2.21e-09
Time:                        09:57:58   Log-Likelihood:                -189.65
No. Observations:                  38   AIC:                             383.3
Df Residuals:                      36   BIC:                             386.6
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        565.5927     92.579      6.109      0.000     377.833     753.352
HtShoes       -4.2621      0.539     -7.907      0.000      -5.355      -3.169
==============================================================================
Omnibus:                        1.370   Durbin-Watson:                   1.913
Prob(Omnibus):                  0.504   Jarque-Bera (JB):                0.786
Skew:                          -0.347   Prob(JB):                        0.675
Kurtosis:                       3.119   Cond. No.                     2.68e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.68e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>El procedimiento es exactamente el mismo, excepto que en cada paso buscamos mejorar el <span class="math notranslate nohighlight">\(\text{BIC}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod_step_bic</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>const      565.592659
HtShoes     -4.262091
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Observamos que este modelo es <em>más pequeño</em>, tiene menos predictores, que el modelo elegido por <span class="math notranslate nohighlight">\(\text{AIC}\)</span>, que es lo que cabría esperar. Tenga en cuenta también que, si bien ambos modelos son diferentes, ninguno utiliza tanto <code class="docutils literal notranslate"><span class="pre">Ht</span></code> y <code class="docutils literal notranslate"><span class="pre">HtShoes</span></code> que están muy correlacionados.</p>
<p>Podemos utilizar la información de la función <code class="docutils literal notranslate"><span class="pre">summary()</span></code> para comparar sus valores <span class="math notranslate nohighlight">\(R^2\)</span> ajustados. Observe que cualquiera de los modelos seleccionados funciona mejor que el modelo completo original.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Comparing adjusted R squared values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_step_aic</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_step_bic</span><span class="o">.</span><span class="n">rsquared_adj</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6000854694116375
0.653142656197544
0.6244148827346013
</pre></div>
</div>
</div>
</div>
<p>También podemos calcular el LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> para ambos modelos seleccionados, así como para el modelo completo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculating LOOCV RMSE (esta función ya la definimos antes)</span>
<span class="k">def</span> <span class="nf">calc_loocv_rmse</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span>  <span class="c1"># This ensures that X is aligned with the model.</span>
    <span class="n">hat_matrix</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="n">loocv_resid</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resid</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">hat_matrix</span><span class="p">))</span>
    <span class="n">loocv_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loocv_resid</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loocv_rmse</span>

<span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod_step_aic</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod_step_bic</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>44.44563802375208
37.58473291770462
37.40564442622142
</pre></div>
</div>
</div>
</div>
<p>Vemos que preferiríamos el modelo elegido mediante <span class="math notranslate nohighlight">\(\text{BIC}\)</span> si utilizamos LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> como nuestra métrica.</p>
</section>
<section id="busqueda-hacia-delante">
<h2>Búsqueda hacia delante<a class="headerlink" href="#busqueda-hacia-delante" title="Permalink to this heading">#</a></h2>
<p>La selección hacia adelante es exactamente lo contrario de la selección hacia atrás. Aquí le decimos a <code class="docutils literal notranslate"><span class="pre">Python</span></code> que empiece con un modelo sin predictores, es decir <code class="docutils literal notranslate"><span class="pre">hipcenter</span> <span class="pre">~</span> <span class="pre">1</span></code>, entonces en cada paso <code class="docutils literal notranslate"><span class="pre">Python</span></code> intentará añadir un predictor hasta que encuentre un buen modelo o alcance <code class="docutils literal notranslate"><span class="pre">centro</span> <span class="pre">de</span> <span class="pre">cadera</span> <span class="pre">~</span> <span class="pre">Edad</span> <span class="pre">+</span> <span class="pre">Peso</span> <span class="pre">+</span> <span class="pre">HtZapatos</span> <span class="pre">+</span> <span class="pre">Ht</span> <span class="pre">+</span> <span class="pre">Sentado</span> <span class="pre">+</span> <span class="pre">Brazo</span> <span class="pre">+</span> <span class="pre">Muslo</span> <span class="pre">+</span> <span class="pre">Pierna</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select features using BIC</span>
<span class="n">sfs_result_aic_f</span> <span class="o">=</span> <span class="n">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Features selected based on BIC</span>
<span class="n">selected_features_aic_f</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs_result_aic_f</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected Features based on BIC: </span><span class="si">{</span><span class="n">selected_features_aic_f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Adjust a full model using only the selected features</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features_aic_f</span><span class="p">]</span>
<span class="n">hipcenter_mod_forw_aic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_selected</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_forw_aic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected Features based on BIC: Index([&#39;Age&#39;, &#39;Ht&#39;, &#39;Leg&#39;], dtype=&#39;object&#39;)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.681
Model:                            OLS   Adj. R-squared:                  0.653
Method:                 Least Squares   F-statistic:                     24.24
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.43e-08
Time:                        09:57:58   Log-Likelihood:                -187.04
No. Observations:                  38   AIC:                             382.1
Df Residuals:                      34   BIC:                             388.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        452.1976    100.948      4.480      0.000     247.046     657.349
Age            0.5807      0.379      1.532      0.135      -0.189       1.351
Ht            -2.3254      1.254     -1.854      0.072      -4.875       0.224
Leg           -6.7390      4.105     -1.642      0.110     -15.081       1.603
==============================================================================
Omnibus:                        0.098   Durbin-Watson:                   1.775
Prob(Omnibus):                  0.952   Jarque-Bera (JB):                0.307
Skew:                           0.044   Prob(JB):                        0.858
Kurtosis:                       2.568   Cond. No.                     3.13e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.13e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>La función que he implementado en  <code class="docutils literal notranslate"><span class="pre">Python</span></code> utiliza <span class="math notranslate nohighlight">\(\text{AIC}\)</span> como su métrica de calidad .</p>
<p>Podemos hacer la misma modificación que la última vez para utilizar en su lugar <span class="math notranslate nohighlight">\(\text{BIC}\)</span> con selección hacia delante.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select features using BIC</span>
<span class="n">sfs_result_bic_f</span> <span class="o">=</span> <span class="n">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Features selected based on BIC</span>
<span class="n">selected_features_bic_f</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs_result_bic_f</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected Features based on BIC: </span><span class="si">{</span><span class="n">selected_features_bic_f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Adjust a full model using only the selected features</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features_bic_f</span><span class="p">]</span>
<span class="n">hipcenter_mod_forw_bic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_selected</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_forw_bic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected Features based on BIC: Index([&#39;Ht&#39;], dtype=&#39;object&#39;)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.638
Model:                            OLS   Adj. R-squared:                  0.628
Method:                 Least Squares   F-statistic:                     63.53
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.83e-09
Time:                        09:57:58   Log-Likelihood:                -189.45
No. Observations:                  38   AIC:                             382.9
Df Residuals:                      36   BIC:                             386.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        556.2553     90.670      6.135      0.000     372.367     740.144
Ht            -4.2650      0.535     -7.970      0.000      -5.350      -3.180
==============================================================================
Omnibus:                        1.707   Durbin-Watson:                   1.890
Prob(Omnibus):                  0.426   Jarque-Bera (JB):                1.028
Skew:                          -0.395   Prob(JB):                        0.598
Kurtosis:                       3.154   Cond. No.                     2.60e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.6e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Podemos comparar el <span class="math notranslate nohighlight">\(R^2\)</span> ajustado de los dos modelos seleccionados, así como su <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> LOOCV Los resultados son muy similares a los obtenidos mediante la selección hacia atrás, aunque los modelos no son exactamente iguales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">rsquared_adj</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6000854694116375
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod_forw_aic</span><span class="o">.</span><span class="n">rsquared_adj</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6533055366110565
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod_forw_bic</span><span class="o">.</span><span class="n">rsquared_adj</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6282373894826558
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>44.44563802375208
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod_forw_aic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>37.625157585910145
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod_forw_bic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>37.25109728478036
</pre></div>
</div>
</div>
</div>
</section>
<section id="busqueda-por-pasos">
<h2>Búsqueda por pasos<a class="headerlink" href="#busqueda-por-pasos" title="Permalink to this heading">#</a></h2>
<p>La búsqueda paso a paso comprueba tanto el avance como el retroceso en cada paso. Considera la adición de cualquier variable que no esté actualmente en el modelo, así como la eliminación de cualquier variable que esté actualmente en el modelo.</p>
<p>Aquí realizamos la búsqueda por pasos utilizando <span class="math notranslate nohighlight">\(\text{AIC}\)</span> como métrica. Comenzamos con el modelo <code class="docutils literal notranslate"><span class="pre">hipcenter</span> <span class="pre">~</span> <span class="pre">1</span></code> y la búsqueda hasta <code class="docutils literal notranslate"><span class="pre">centro</span> <span class="pre">de</span> <span class="pre">cadera</span> <span class="pre">~</span> <span class="pre">Edad</span> <span class="pre">+</span> <span class="pre">Peso</span> <span class="pre">+</span> <span class="pre">HtShoes</span> <span class="pre">+</span> <span class="pre">Ht</span> <span class="pre">+</span> <span class="pre">Sentado</span> <span class="pre">+</span> <span class="pre">Brazo</span> <span class="pre">+</span> <span class="pre">Muslo</span> <span class="pre">+</span> <span class="pre">Pierna</span></code>. Observe que en muchos de los pasos, algunas filas empiezan por <code class="docutils literal notranslate"><span class="pre">-</span></code>, mientras que otras empiezan por <code class="docutils literal notranslate"><span class="pre">+</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select features using BIC</span>
<span class="n">sfs_result_both_f</span> <span class="o">=</span> <span class="n">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">floating</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Features selected based on BIC</span>
<span class="n">selected_features_both_f</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs_result_both_f</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected Features based on BIC: </span><span class="si">{</span><span class="n">selected_features_both_f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Adjust a full model using only the selected features</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features_both_f</span><span class="p">]</span>
<span class="n">hipcenter_mod_both_aic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_selected</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_both_aic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected Features based on BIC: Index([&#39;Age&#39;, &#39;Ht&#39;, &#39;Leg&#39;], dtype=&#39;object&#39;)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.681
Model:                            OLS   Adj. R-squared:                  0.653
Method:                 Least Squares   F-statistic:                     24.24
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.43e-08
Time:                        09:58:30   Log-Likelihood:                -187.04
No. Observations:                  38   AIC:                             382.1
Df Residuals:                      34   BIC:                             388.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        452.1976    100.948      4.480      0.000     247.046     657.349
Age            0.5807      0.379      1.532      0.135      -0.189       1.351
Ht            -2.3254      1.254     -1.854      0.072      -4.875       0.224
Leg           -6.7390      4.105     -1.642      0.110     -15.081       1.603
==============================================================================
Omnibus:                        0.098   Durbin-Watson:                   1.775
Prob(Omnibus):                  0.952   Jarque-Bera (JB):                0.307
Skew:                           0.044   Prob(JB):                        0.858
Kurtosis:                       2.568   Cond. No.                     3.13e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.13e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Podríamos volver a utilizar <span class="math notranslate nohighlight">\(\text{BIC}\)</span> como métrica.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select features using BIC</span>
<span class="n">sfs_result_both_f_bic</span> <span class="o">=</span> <span class="n">select_features_using_criterion</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">floating</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Features selected based on BIC</span>
<span class="n">selected_features_both_f_bic</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs_result_both_f_bic</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Selected Features based on BIC: </span><span class="si">{</span><span class="n">selected_features_both_f_bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Adjust a full model using only the selected features</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">selected_features_both_f_bic</span><span class="p">]</span>
<span class="n">hipcenter_mod_both_bic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_selected</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print the model summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hipcenter_mod_both_bic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Selected Features based on BIC: Index([&#39;Ht&#39;], dtype=&#39;object&#39;)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.638
Model:                            OLS   Adj. R-squared:                  0.628
Method:                 Least Squares   F-statistic:                     63.53
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.83e-09
Time:                        09:58:51   Log-Likelihood:                -189.45
No. Observations:                  38   AIC:                             382.9
Df Residuals:                      36   BIC:                             386.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        556.2553     90.670      6.135      0.000     372.367     740.144
Ht            -4.2650      0.535     -7.970      0.000      -5.350      -3.180
==============================================================================
Omnibus:                        1.707   Durbin-Watson:                   1.890
Prob(Omnibus):                  0.426   Jarque-Bera (JB):                1.028
Skew:                          -0.395   Prob(JB):                        0.598
Kurtosis:                       3.154   Cond. No.                     2.60e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.6e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Las comparaciones ajustadas <span class="math notranslate nohighlight">\(R^2\)</span> y LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> son similares a las anteriores y posteriores, lo que no es en absoluto sorprendente, ya que algunos de los modelos seleccionados son los mismos que antes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod</span><span class="o">.</span><span class="n">rsquared</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6865534760253376
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod_both_aic</span><span class="o">.</span><span class="n">rsquared</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6814158985074573
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hipcenter_mod_both_bic</span><span class="o">.</span><span class="n">rsquared</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6382850276047463
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod_both_aic</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">hipcenter_mod_both_bic</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>44.44563802375208
37.625157585910145
37.25109728478036
</pre></div>
</div>
</div>
</div>
</section>
<section id="busqueda-exhaustiva">
<h2>Búsqueda exhaustiva<a class="headerlink" href="#busqueda-exhaustiva" title="Permalink to this heading">#</a></h2>
<p>Las búsquedas hacia atrás, hacia delante y por pasos son útiles, pero tienen un problema obvio. Al no comprobar todos los modelos posibles, a veces pasan por alto el mejor modelo posible. Con un número extremadamente grande de predictores, a veces esto es necesario, ya que comprobar todos los modelos posibles llevaría mucho tiempo, incluso con los ordenadores actuales.</p>
<p>Sin embargo, con un conjunto de datos de tamaño razonable, no es demasiado difícil comprobar todos los modelos posibles. Para ello, utilizaremos la función <code class="docutils literal notranslate"><span class="pre">ExhaustiveFeatureSelector()</span></code> del paquete <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> de <code class="docutils literal notranslate"><span class="pre">Python</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">ExhaustiveFeatureSelector</span> <span class="k">as</span> <span class="n">EFS</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Assuming seatpos is your DataFrame</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">seatpos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;hipcenter&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">seatpos</span><span class="p">[</span><span class="s1">&#39;hipcenter&#39;</span><span class="p">]</span>

<span class="c1"># Define the model</span>
<span class="n">sm_model</span> <span class="o">=</span> <span class="n">SMWrapper</span><span class="p">()</span>

<span class="n">efs</span> <span class="o">=</span> <span class="n">EFS</span><span class="p">(</span><span class="n">sm_model</span><span class="p">,</span> 
          <span class="n">min_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">max_features</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
          <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span>
          <span class="n">print_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">cv</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">efs</span> <span class="o">=</span> <span class="n">efs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy score: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">efs</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best subset (indices):&#39;</span><span class="p">,</span> <span class="n">efs</span><span class="o">.</span><span class="n">best_idx_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best subset (corresponding names):&#39;</span><span class="p">,</span> <span class="n">efs</span><span class="o">.</span><span class="n">best_feature_names_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Features: 255/255
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy score: -1085.84
Best subset (indices): (0, 1, 2, 3, 4, 5, 6, 7)
Best subset (corresponding names): (&#39;Age&#39;, &#39;Weight&#39;, &#39;HtShoes&#39;, &#39;Ht&#39;, &#39;Seated&#39;, &#39;Arm&#39;, &#39;Thigh&#39;, &#39;Leg&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">efs</span><span class="o">.</span><span class="n">get_metric_dict</span><span class="p">())</span><span class="o">.</span><span class="n">T</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;avg_score&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_methods.py:262: RuntimeWarning: Degrees of freedom &lt;= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
c:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_idx</th>
      <th>cv_scores</th>
      <th>avg_score</th>
      <th>feature_names</th>
      <th>ci_bound</th>
      <th>std_dev</th>
      <th>std_err</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>254</th>
      <td>(0, 1, 2, 3, 4, 5, 6, 7)</td>
      <td>[-1085.8364079047815]</td>
      <td>-1085.836408</td>
      <td>(Age, Weight, HtShoes, Ht, Seated, Arm, Thigh,...</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>250</th>
      <td>(0, 1, 2, 4, 5, 6, 7)</td>
      <td>[-1085.9683566079034]</td>
      <td>-1085.968357</td>
      <td>(Age, Weight, HtShoes, Seated, Arm, Thigh, Leg)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>252</th>
      <td>(0, 2, 3, 4, 5, 6, 7)</td>
      <td>[-1086.073071567894]</td>
      <td>-1086.073072</td>
      <td>(Age, HtShoes, Ht, Seated, Arm, Thigh, Leg)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>237</th>
      <td>(0, 2, 4, 5, 6, 7)</td>
      <td>[-1086.2605637725326]</td>
      <td>-1086.260564</td>
      <td>(Age, HtShoes, Seated, Arm, Thigh, Leg)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>249</th>
      <td>(0, 1, 2, 3, 5, 6, 7)</td>
      <td>[-1086.5901656539493]</td>
      <td>-1086.590166</td>
      <td>(Age, Weight, HtShoes, Ht, Arm, Thigh, Leg)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>33</th>
      <td>(5, 6)</td>
      <td>[-2029.811290062231]</td>
      <td>-2029.81129</td>
      <td>(Arm, Thigh)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>(1,)</td>
      <td>[-2043.7773373912266]</td>
      <td>-2043.777337</td>
      <td>(Weight,)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>(6,)</td>
      <td>[-2253.3848949042167]</td>
      <td>-2253.384895</td>
      <td>(Thigh,)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>(5,)</td>
      <td>[-2278.2685835615866]</td>
      <td>-2278.268584</td>
      <td>(Arm,)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>(0,)</td>
      <td>[-3318.3569492310544]</td>
      <td>-3318.356949</td>
      <td>(Age,)</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>255 rows × 7 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract models for each subset</span>
<span class="n">all_models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">efs</span><span class="o">.</span><span class="n">subsets_</span><span class="p">:</span>
    <span class="n">X_sub</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">efs</span><span class="o">.</span><span class="n">subsets_</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;feature_idx&#39;</span><span class="p">])]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_sub</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">all_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">all_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.042
Model:                            OLS   Adj. R-squared:                  0.015
Method:                 Least Squares   F-statistic:                     1.582
Date:                Mon, 09 Oct 2023   Prob (F-statistic):              0.217
Time:                        10:00:09   Log-Likelihood:                -207.96
No. Observations:                  38   AIC:                             419.9
Df Residuals:                      36   BIC:                             423.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       -192.9645     24.302     -7.940      0.000    -242.250    -143.679
Age            0.7963      0.633      1.258      0.217      -0.488       2.080
==============================================================================
Omnibus:                        0.600   Durbin-Watson:                   2.357
Prob(Omnibus):                  0.741   Jarque-Bera (JB):                0.713
Skew:                           0.189   Prob(JB):                        0.700
Kurtosis:                       2.446   Cond. No.                         97.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>Usando <code class="docutils literal notranslate"><span class="pre">best_feature_names_</span></code> nos da el mejor modelo, según <span class="math notranslate nohighlight">\(neg_mean_squared_error\)</span>, para un modelo de cada tamaño posible, en este caso, que van de uno a ocho predictores. Por ejemplo, el mejor modelo con cuatro predictores (<span class="math notranslate nohighlight">\(p = 5\)</span>) utilizaría <code class="docutils literal notranslate"><span class="pre">Edad</span></code>, <code class="docutils literal notranslate"><span class="pre">HtZapatos</span></code>, <code class="docutils literal notranslate"><span class="pre">Muslo</span></code> y <code class="docutils literal notranslate"><span class="pre">Pierna</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate RSS and Adjusted R^2 for each model</span>
<span class="n">rss</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">ssr</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">all_models</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[126097.56407078005, 77663.53882086661, 48105.38441661299, 47615.79324780125, 61247.41628906701, 86574.20617534031, 85628.62600636024, 50070.9628991935, 68922.99928714373, 45434.04964306016, 45262.28508705717, 60364.32690032101, 60472.14753872091, 76714.2837836853, 46176.34875315023, 47947.824315011836, 47419.932233639905, 59479.243686141264, 72750.31582232233, 67788.91032322388, 49890.20882440744, 47590.03809691491, 48021.08546909509, 48046.78886299168, 48052.721108576356, 45066.560944843535, 47514.22962984801, 47540.01235943727, 47611.16342312447, 44834.69380438051, 57711.214026767426, 56726.291357248556, 46791.591538507346, 77132.82902236478, 50050.04173672554, 48617.13692315454, 45433.00353802825, 45262.03521245436, 57277.21806892544, 54219.00472129215, 57963.26554735053, 45534.17496128101, 45249.907764407755, 44877.56328665762, 44354.03356609099, 45076.751776148376, 41957.79193040662, 44777.44061814211, 44353.68903943072, 45070.478072506434, 41938.08901039699, 50883.15307064926, 54428.776064080914, 44245.624622991716, 56420.26516009671, 44523.92336547447, 43825.681347236736, 47398.86115742054, 47869.69995270792, 47920.559277168286, 47907.211320924864, 44656.50912926164, 47326.973352332614, 47383.842701379035, 47419.26503664898, 44389.59081949875, 57381.391043444244, 55884.48798407443, 46706.62813420781, 67047.73783448627, 49820.12260984741, 48546.38437455601, 47500.52930337248, 47516.82299067898, 47588.823685402516, 44800.58284290955, 47900.795024876104, 47995.91287613555, 45060.01460734599, 47951.43347964493, 44557.7938228009, 44986.0263055716, 47363.91898758884, 47513.438099303676, 44833.57382268182, 47518.65713385881, 44311.97409276509, 44801.73302053219, 55730.14259436604, 46748.78010753899, 45990.46425900819, 48226.13818810425, 45249.70788957534, 44865.42867079442, 44353.73994612947, 45063.54120177199, 41919.068134733796, 44774.245915051746, 44351.904685447655, 45066.82751523321, 41891.2618941745, 50121.307435908806, 52315.18164670651, 44204.35734253592, 51474.1947258461, 44204.382239562576, 43417.49053948478, 44716.23182352917, 44305.063528998806, 45015.12313007291, 41924.706954739144, 44035.33920278124, 44679.37867389247, 41813.81261845892, 44184.06907406264, 41657.6050236806, 41485.014556321854, 44084.82367620772, 44705.72209298986, 41816.8278227809, 44266.27295695668, 41681.002354737255, 41565.38105057629, 49259.330532806765, 43241.157276880294, 42676.14140973575, 43166.43064595479, 47316.19943638035, 47364.03823961699, 47398.839184518685, 44363.14507274795, 47798.24311978831, 47852.02735342147, 44642.48258399122, 47851.42859620102, 44283.58678129171, 44598.57847229463, 47237.30923229539, 47322.412649362625, 44384.06422150645, 47376.64316794965, 44010.76593108678, 44374.3360286845, 55400.18139521405, 46688.78655951921, 45863.33060828719, 48048.89033750173, 47356.19267527365, 47498.02788690057, 44796.845115738935, 47503.72774903491, 44284.73487928781, 44779.3194100253, 47843.79076691862, 44545.310607899206, 44958.557556217595, 44289.764772248556, 47361.64538042345, 44286.84052632422, 44792.393912470354, 44143.250381032834, 45664.47697102136, 44711.96467492484, 44303.49495033827, 45009.41660097452, 41878.61001010137, 44033.06119593642, 44652.664944416356, 41791.87582609064, 44182.10318072144, 41615.63638857455, 41473.52820264064, 44084.58413520359, 44696.08711430877, 41786.801914329975, 44266.158982171255, 41634.181856453855, 41551.4166803633, 48510.19340175568, 43231.74969921076, 42639.488668181126, 42904.88779776027, 43986.26086745877, 44605.56610811937, 41782.18430265994, 44173.486238104386, 41650.07601853987, 41482.37824920715, 43939.287989298784, 41563.064962352684, 41434.368179712474, 41312.99813654719, 44052.755138516826, 41606.712853677775, 41531.71248314063, 41403.84762093688, 42253.377977898235, 47230.45392253931, 47308.56006947926, 44353.36533706185, 47361.07647705254, 43988.49054576788, 44354.772711244266, 47757.545521346685, 44280.627645524124, 44561.901947036014, 44076.84938677859, 47237.3058436447, 44001.20446642234, 44360.03972079191, 43899.611926101104, 45618.213492411654, 47355.24323100345, 44266.76898892196, 44767.27397763285, 44137.95405208734, 44289.76455990313, 44140.5816172101, 43985.61420840872, 44590.82619317096, 41754.67054207547, 44172.65114288828, 41604.37624562303, 41472.23474712669, 43931.45014126116, 41535.881543262236, 41427.29950362615, 41297.31892570539, 44050.79534789646, 41573.360305051385, 41521.165391088376, 41386.77967197084, 42238.63754262536, 43921.452090885774, 41554.78561251984, 41433.1536298453, 41302.97755696593, 41277.90142335624, 41384.24976766875, 47230.10496498702, 43982.81345213814, 44337.38743246313, 43893.93847527601, 44075.321666060285, 43899.414394578576, 44135.99143054256, 43916.63184650537, 41524.545026509804, 41426.75844252076, 41290.42629485007, 41266.79755110032, 41370.21415688665, 41270.77671957997, 43893.900416861245, 41261.783500381694]
</pre></div>
</div>
</div>
</div>
<p>Podemos obtener el <span class="math notranslate nohighlight">\({RSS}\)</span> para cada uno de estos modelos utilizando `$rss``. Obsérvese que son decrecientes, ya que los modelos van de pequeños a grandes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">adj_r2</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">rsquared_adj</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">all_models</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">adj_r2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.015487165049047125, 0.3936381615275577, 0.6244148827346013, 0.6282373894826558, 0.5218078327799887, 0.3240676948567376, 0.331450357794726, 0.6090685335924584, 0.44650549977473253, 0.6351363570870259, 0.6365157331743202, 0.5152369558101907, 0.5143710891041222, 0.38393664521052673, 0.6291752341062156, 0.6149491848785993, 0.6191884862261008, 0.5223447238463617, 0.4157697704136666, 0.45561293866723707, 0.599350630631211, 0.6178224304718158, 0.6143608523004378, 0.6141544380801336, 0.6141067984628932, 0.6380875196229224, 0.6184312195556633, 0.6182241682204623, 0.6176527809789993, 0.6399495568000131, 0.5365431003363574, 0.5444526412205941, 0.6242344523422108, 0.38057546693766275, 0.5980670730548134, 0.6095741888486818, 0.6244137214297077, 0.6258270851115066, 0.5264997797611946, 0.551781466659798, 0.5208283515200802, 0.623577355893686, 0.6259273404925221, 0.629005443762259, 0.6333333676096, 0.6273587891791154, 0.653142656197544, 0.6298331394358851, 0.6333362157480924, 0.627410652732485, 0.6533055366110565, 0.5793583383116763, 0.5500473255761522, 0.6342295643901699, 0.5335840517413105, 0.6319289199053209, 0.6377011581627323, 0.6081623382947892, 0.6042700006292752, 0.6038495559543224, 0.6039599010519736, 0.6308328577977267, 0.6087566215489211, 0.6082864931068255, 0.6079936631804224, 0.6330394223397826, 0.5256386009532741, 0.5380132230488308, 0.6138849012962495, 0.4457286910663233, 0.588146215486586, 0.5986759750536672, 0.6073218664436323, 0.6071871695294273, 0.6065919530374544, 0.6296418269229238, 0.6040129433905268, 0.6032266216200493, 0.6274971522732267, 0.6035943246046644, 0.6316489190682628, 0.6281087999468814, 0.6084511987818859, 0.6072151518034574, 0.6293690965649432, 0.607172007005323, 0.6336810655347853, 0.629632318616201, 0.539289167979184, 0.6135364386905447, 0.619805296246752, 0.6013233913376261, 0.6145935077549447, 0.6178665389558191, 0.6222247583939582, 0.6161791500378577, 0.6429616507709827, 0.6186431721692264, 0.6222403899066181, 0.616151159436314, 0.6431984855735993, 0.5731005085657161, 0.554414567739892, 0.6234970986573234, 0.561577527111558, 0.6234968866012427, 0.6301990993678714, 0.619137297072067, 0.6226393512843071, 0.6165915425160666, 0.6429136231053906, 0.6249366811741344, 0.6194511873443382, 0.6438581462669524, 0.6236699004443558, 0.6451886172019794, 0.6466586263958951, 0.6245152058052672, 0.6192268118683056, 0.6438324647902016, 0.6229697432138952, 0.6449893345168304, 0.6459741188026036, 0.5804422464490087, 0.6317009871672881, 0.6365134112375248, 0.6323374582086607, 0.5969925265452592, 0.5965850678851143, 0.5962886569086256, 0.6221446518688951, 0.5928868035738641, 0.5924287057476145, 0.6197654433528212, 0.5924338055577747, 0.6228222757358599, 0.6201393889642584, 0.5976644600946001, 0.5969396065877003, 0.6219664766691029, 0.5964777076072194, 0.6251459797284998, 0.6220493348541887, 0.528138620621058, 0.6023364062593675, 0.6093670833069078, 0.590751959627343, 0.5966518911105954, 0.5954438344415143, 0.6184506874202497, 0.5953952868602386, 0.6228124970114866, 0.6185999595609604, 0.5924988594361176, 0.6205930887056164, 0.6170733299223987, 0.6227696557757574, 0.5966054486458139, 0.6227945625302256, 0.6184885997674896, 0.6240175665406107, 0.6110607844683787, 0.6072728180709978, 0.6108606086099844, 0.6046601514721095, 0.6321595659697219, 0.6132359613156165, 0.6077936767767393, 0.6329213949770379, 0.611926852241585, 0.6344693926582774, 0.635717598765896, 0.6127834099931638, 0.6074122784969036, 0.6329659616401583, 0.6111885487859401, 0.6343064986903932, 0.6350334659485807, 0.5739111066128875, 0.6202742743990242, 0.6254763943992012, 0.6231452632801859, 0.613647031624202, 0.6082073690263539, 0.6330065203951045, 0.6120025390877221, 0.6341668924456096, 0.6356398644530946, 0.6140596175215337, 0.6349311533574595, 0.6360615605121682, 0.6371276133096566, 0.6130629797294369, 0.6345477724552363, 0.6352065376063281, 0.6363296375945775, 0.6288677945403026, 0.5851516880306348, 0.5844656433175011, 0.6104225724695003, 0.584004365874176, 0.6136274472627159, 0.610410210809694, 0.5805219832139693, 0.6110614633978082, 0.6085908928338678, 0.6128513480937141, 0.5850915042367424, 0.6135157746429551, 0.6103639480737155, 0.6144081119033608, 0.5993127888751059, 0.5840556021439085, 0.6111831907965539, 0.606787009255731, 0.6123146357576785, 0.610981209383884, 0.6122915565605666, 0.6011898957922729, 0.5957025413684566, 0.6214174836539844, 0.5994940636282144, 0.6227801765504852, 0.6239782811065748, 0.6016809922030468, 0.6234012064003076, 0.6243857013385202, 0.6255642131942362, 0.600598909455051, 0.6230613928234437, 0.6235346352562554, 0.6247530877801046, 0.6170294369392588, 0.6017716428301629, 0.6232298064104187, 0.6243326229689322, 0.6255129073442827, 0.6257402683948894, 0.6247760259933479, 0.5717726483576644, 0.601215289774379, 0.5980004276293887, 0.6020211041166754, 0.600376533487845, 0.6019714548850574, 0.599826451022683, 0.5885425254074108, 0.6109541257642335, 0.6118702939493939, 0.613147597759541, 0.6133689768321677, 0.6124000606457782, 0.6133316957715587, 0.5887554974920628, 0.6000854694116375]
</pre></div>
</div>
</div>
</div>
<p>Para averiguar qué modelo tiene el mayor <span class="math notranslate nohighlight">\(R^2\)</span> ajustado podemos utilizar la función <code class="docutils literal notranslate"><span class="pre">np.argmax</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># best rsquared of the models and its index and the model</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">adj_r2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">adj_r2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">all_models</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">adj_r2</span><span class="p">)]</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6533055366110565
50
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.681
Model:                            OLS   Adj. R-squared:                  0.653
Method:                 Least Squares   F-statistic:                     24.24
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.43e-08
Time:                        09:58:52   Log-Likelihood:                -187.04
No. Observations:                  38   AIC:                             382.1
Df Residuals:                      34   BIC:                             388.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        452.1976    100.948      4.480      0.000     247.046     657.349
Age            0.5807      0.379      1.532      0.135      -0.189       1.351
Ht            -2.3254      1.254     -1.854      0.072      -4.875       0.224
Leg           -6.7390      4.105     -1.642      0.110     -15.081       1.603
==============================================================================
Omnibus:                        0.098   Durbin-Watson:                   1.775
Prob(Omnibus):                  0.952   Jarque-Bera (JB):                0.307
Skew:                           0.044   Prob(JB):                        0.858
Kurtosis:                       2.568   Cond. No.                     3.13e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.13e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>A continuación, podemos extraer los predictores de ese modelo  <code class="docutils literal notranslate"><span class="pre">print(all_models[np.argmax(adj_r2)].summary())</span></code></p>
<p>Ahora vamos a calcular <span class="math notranslate nohighlight">\(\text{AIC}\)</span> y <span class="math notranslate nohighlight">\(\text{BIC}\)</span><span class="math notranslate nohighlight">\( para cada uno de los modelos con el mejor \)</span>\text{RSS}<span class="math notranslate nohighlight">\(. Para ello, necesitaremos tanto \)</span>n<span class="math notranslate nohighlight">\( como el \)</span>p$ para el mayor modelo posible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Derive AIC and BIC values for each model</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">aic_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">aic</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">all_models</span><span class="p">]</span>
<span class="n">bic_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">bic</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">all_models</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>A continuación, podemos extraer los predictores del modelo con la mejor
<span class="math notranslate nohighlight">\(\text{AIC}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract the best model based on AIC and BIC</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">aic_values</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">bic_values</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">all_models</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">aic_values</span><span class="p">)]</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">all_models</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">bic_values</span><span class="p">)]</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>382.08114451971556
386.18117826555755
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.681
Model:                            OLS   Adj. R-squared:                  0.653
Method:                 Least Squares   F-statistic:                     24.24
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.43e-08
Time:                        09:59:24   Log-Likelihood:                -187.04
No. Observations:                  38   AIC:                             382.1
Df Residuals:                      34   BIC:                             388.6
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        452.1976    100.948      4.480      0.000     247.046     657.349
Age            0.5807      0.379      1.532      0.135      -0.189       1.351
Ht            -2.3254      1.254     -1.854      0.072      -4.875       0.224
Leg           -6.7390      4.105     -1.642      0.110     -15.081       1.603
==============================================================================
Omnibus:                        0.098   Durbin-Watson:                   1.775
Prob(Omnibus):                  0.952   Jarque-Bera (JB):                0.307
Skew:                           0.044   Prob(JB):                        0.858
Kurtosis:                       2.568   Cond. No.                     3.13e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.13e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              hipcenter   R-squared:                       0.638
Model:                            OLS   Adj. R-squared:                  0.628
Method:                 Least Squares   F-statistic:                     63.53
Date:                Mon, 09 Oct 2023   Prob (F-statistic):           1.83e-09
Time:                        09:59:24   Log-Likelihood:                -189.45
No. Observations:                  38   AIC:                             382.9
Df Residuals:                      36   BIC:                             386.2
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        556.2553     90.670      6.135      0.000     372.367     740.144
Ht            -4.2650      0.535     -7.970      0.000      -5.350      -3.180
==============================================================================
Omnibus:                        1.707   Durbin-Watson:                   1.890
Prob(Omnibus):                  0.426   Jarque-Bera (JB):                1.028
Skew:                          -0.395   Prob(JB):                        0.598
Kurtosis:                       3.154   Cond. No.                     2.60e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.6e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract parameter counts for each model</span>
<span class="n">p_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">df_model</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">all_models</span><span class="p">]</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">aic_values</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;p_values and aic_values must have the same length&quot;</span><span class="p">)</span>

<span class="c1"># Create a dictionary to store the best AIC for each parameter count</span>
<span class="n">best_aic_per_p</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Update dictionary with best AIC values for each parameter count</span>
<span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">aic</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">aic_values</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">best_aic_per_p</span> <span class="ow">or</span> <span class="n">aic</span> <span class="o">&lt;</span> <span class="n">best_aic_per_p</span><span class="p">[</span><span class="n">p</span><span class="p">]:</span>
        <span class="n">best_aic_per_p</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">aic</span>

<span class="c1"># Extract unique parameter counts and their corresponding best AIC values</span>
<span class="n">unique_p_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">best_aic_per_p</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">unique_aic_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">best_aic_per_p</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="c1"># Plot AIC vs Model Complexity with unique values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">unique_p_values</span><span class="p">,</span> <span class="n">unique_aic_values</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;dodgerblue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Parameters&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AIC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;AIC vs Model Complexity&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1d3b692d64c3abea911ffff34b5d8c6719a04da1546d94403b9a4c5cbc61554e.png" src="../_images/1d3b692d64c3abea911ffff34b5d8c6719a04da1546d94403b9a4c5cbc61554e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assuming aic_values contains the AIC values of all the models in all_models</span>

<span class="c1"># Create a dictionary to store the best AIC for each parameter count</span>
<span class="n">best_aic_per_p</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Update dictionary with best AIC values for each parameter count</span>
<span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">aic</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">aic_values</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">best_aic_per_p</span> <span class="ow">or</span> <span class="n">aic</span> <span class="o">&lt;</span> <span class="n">best_aic_per_p</span><span class="p">[</span><span class="n">p</span><span class="p">]:</span>
        <span class="n">best_aic_per_p</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">aic</span>

<span class="c1"># Extract unique parameter counts and their corresponding best AIC values</span>
<span class="n">unique_p_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">best_aic_per_p</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">unique_aic_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">best_aic_per_p</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="c1"># Plot AIC vs Model Complexity with unique values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">unique_p_values</span><span class="p">,</span> <span class="n">unique_aic_values</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;dodgerblue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Parameters&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AIC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;AIC vs Model Complexity&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="ne">c</span>:\Users\a.tabaresp\MEL_202302\Week_10\10-1-Selection.ipynb Celda 86 line 7
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y151sZmlsZQ%3D%3D?line=3&#39;</span><span class="o">&gt;</span><span class="mi">4</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">best_aic_per_p</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y151sZmlsZQ%3D%3D?line=5&#39;</span><span class="o">&gt;</span><span class="mi">6</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="c1"># Update dictionary with best AIC values for each parameter count</span>
<span class="o">----&gt;</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y151sZmlsZQ%3D%3D?line=6&#39;</span><span class="o">&gt;</span><span class="mi">7</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">aic</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">aic_values</span><span class="p">):</span>
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y151sZmlsZQ%3D%3D?line=7&#39;</span><span class="o">&gt;</span><span class="mi">8</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>     <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">best_aic_per_p</span> <span class="ow">or</span> <span class="n">aic</span> <span class="o">&lt;</span> <span class="n">best_aic_per_p</span><span class="p">[</span><span class="n">p</span><span class="p">]:</span>
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y151sZmlsZQ%3D%3D?line=8&#39;</span><span class="o">&gt;</span><span class="mi">9</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>         <span class="n">best_aic_per_p</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">aic</span>

<span class="ne">NameError</span>: name &#39;p_values&#39; is not defined
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="terminos-de-orden-superior">
<h1>Términos de orden superior<a class="headerlink" href="#terminos-de-orden-superior" title="Permalink to this heading">#</a></h1>
<p>Hasta ahora sólo hemos permitido términos de primer orden en nuestros modelos. Volvamos al conjunto de datos <code class="docutils literal notranslate"><span class="pre">autompg</span></code> para explorar los términos de orden superior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read data frame from the web</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;</span>
<span class="n">autompg</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">quotechar</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">na_values</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;?&quot;</span><span class="p">])</span>

<span class="c1"># Assign headers to the dataframe</span>
<span class="n">autompg</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mpg&quot;</span><span class="p">,</span> <span class="s2">&quot;cyl&quot;</span><span class="p">,</span> <span class="s2">&quot;disp&quot;</span><span class="p">,</span> <span class="s2">&quot;hp&quot;</span><span class="p">,</span> <span class="s2">&quot;wt&quot;</span><span class="p">,</span> <span class="s2">&quot;acc&quot;</span><span class="p">,</span> <span class="s2">&quot;year&quot;</span><span class="p">,</span> <span class="s2">&quot;origin&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">]</span>

<span class="c1"># Remove rows with missing &#39;hp&#39; data</span>
<span class="n">autompg</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;hp&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">notna</span><span class="p">()]</span>

<span class="c1"># Remove &#39;plymouth reliant&#39;</span>
<span class="n">autompg</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;plymouth reliant&quot;</span><span class="p">]</span>

<span class="c1"># Create row names based on the engine, year, and name</span>
<span class="n">autompg</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;cyl&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; cylinder &quot;</span> <span class="o">+</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>

<span class="c1"># Drop the &#39;name&#39; column</span>
<span class="n">autompg</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Convert horsepower from object to numeric</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;hp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;hp&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>

<span class="c1"># Create a dummy variable for foreign vs. domestic cars. Domestic = 1.</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;domestic&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;origin&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Remove 3 and 5 cylinder cars</span>
<span class="n">autompg</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="o">~</span><span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;cyl&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])]</span>

<span class="c1"># Change &#39;cyl&#39; to a category variable</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;cyl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;cyl&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>

<span class="c1"># Display the structure (similar to str in R)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">autompg</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 383 entries, 8 cylinder 70 chevrolet chevelle malibu to 4 cylinder 82 chevy s-10
Data columns (total 9 columns):
 #   Column    Non-Null Count  Dtype   
---  ------    --------------  -----   
 0   mpg       383 non-null    float64 
 1   cyl       383 non-null    category
 2   disp      383 non-null    float64 
 3   hp        383 non-null    float64 
 4   wt        383 non-null    float64 
 5   acc       383 non-null    float64 
 6   year      383 non-null    int64   
 7   origin    383 non-null    int64   
 8   domestic  383 non-null    int32   
dtypes: category(1), float64(5), int32(1), int64(2)
memory usage: 25.9+ KB
None
</pre></div>
</div>
</div>
</div>
<p>Recordemos que tenemos dos variables factoriales, <code class="docutils literal notranslate"><span class="pre">cyl</span></code> y <code class="docutils literal notranslate"><span class="pre">doméstica</span></code>. La variable <code class="docutils literal notranslate"><span class="pre">cyl</span></code> tiene tres niveles, mientras que la variable <code class="docutils literal notranslate"><span class="pre">doméstico</span></code> sólo tiene dos. Por lo tanto, la variable <code class="docutils literal notranslate"><span class="pre">cyl</span></code> se codificará utilizando dos variables ficticias, mientras que la variable <code class="docutils literal notranslate"><span class="pre">doméstica</span></code> sólo necesitará una. Tendremos esto en cuenta más adelante.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Scatterplot matrix</span>
<span class="n">pd</span><span class="o">.</span><span class="n">plotting</span><span class="o">.</span><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">autompg</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;dodgerblue&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/799323d19d16b732169142921c3f6fac3835fb77e154dd9aff39d3fcc8b69753.png" src="../_images/799323d19d16b732169142921c3f6fac3835fb77e154dd9aff39d3fcc8b69753.png" />
</div>
</div>
<p>Utilizaremos el gráfico <code class="docutils literal notranslate"><span class="pre">pairs()</span></code> para determinar qué variables pueden beneficiarse de una relación cuadrática con la respuesta. También consideraremos todas las posibles interacciones bidireccionales. No consideraremos ninguna de tres órdenes o superior. Por ejemplo, no consideraremos la interacción entre los términos de primer orden y los términos cuadráticos añadidos.</p>
<p>Así que ahora, vamos a ajustar este modelo bastante grande. Usaremos una respuesta logarítmica. Vamos a crear términos cuadraticos y todas las posibles interacciones entre dos variables con los datos originales del modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adding squared features</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;disp_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;disp&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;hp_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;hp&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;wt_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;wt&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;acc_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">autompg</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>

<span class="c1"># Formula for the full model</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;np.log(mpg) ~ cyl + disp + hp + wt + acc + year + domestic + </span><span class="se">\</span>
<span class="s1">           disp_sq + hp_sq + wt_sq + acc_sq + </span><span class="se">\</span>
<span class="s1">           (cyl + disp + hp + wt + acc + year + domestic)**2&#39;</span>
           
<span class="n">autompg_big_mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">autompg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">autompg</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 383 entries, 8 cylinder 70 chevrolet chevelle malibu to 4 cylinder 82 chevy s-10
Data columns (total 13 columns):
 #   Column    Non-Null Count  Dtype   
---  ------    --------------  -----   
 0   mpg       383 non-null    float64 
 1   cyl       383 non-null    category
 2   disp      383 non-null    float64 
 3   hp        383 non-null    float64 
 4   wt        383 non-null    float64 
 5   acc       383 non-null    float64 
 6   year      383 non-null    int64   
 7   origin    383 non-null    int64   
 8   domestic  383 non-null    int32   
 9   disp_sq   383 non-null    float64 
 10  hp_sq     383 non-null    float64 
 11  wt_sq     383 non-null    float64 
 12  acc_sq    383 non-null    float64 
dtypes: category(1), float64(9), int32(1), int64(2)
memory usage: 46.0+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:            np.log(mpg)   R-squared:                       0.922
Model:                            OLS   Adj. R-squared:                  0.913
Method:                 Least Squares   F-statistic:                     106.6
Date:                Mon, 09 Oct 2023   Prob (F-statistic):          7.76e-167
Time:                        10:12:01   Log-Likelihood:                 355.70
No. Observations:                 383   AIC:                            -633.4
Df Residuals:                     344   BIC:                            -479.4
Df Model:                          38                                         
Covariance Type:            nonrobust                                         
=====================================================================================
                        coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept             3.8615      1.942      1.988      0.048       0.042       7.681
cyl[T.6]              1.0117      0.714      1.416      0.158      -0.394       2.417
cyl[T.8]              0.6175      0.703      0.878      0.380      -0.765       2.001
disp                 -0.0201      0.008     -2.613      0.009      -0.035      -0.005
cyl[T.6]:disp      7.944e-05      0.002      0.044      0.965      -0.003       0.004
cyl[T.8]:disp        -0.0014      0.003     -0.450      0.653      -0.008       0.005
hp                    0.0368      0.016      2.323      0.021       0.006       0.068
cyl[T.6]:hp          -0.0006      0.003     -0.240      0.811      -0.006       0.004
cyl[T.8]:hp          -0.0007      0.004     -0.172      0.864      -0.009       0.007
wt                   -0.0007      0.001     -0.974      0.331      -0.002       0.001
cyl[T.6]:wt        9.321e-05      0.000      0.733      0.464      -0.000       0.000
cyl[T.8]:wt           0.0001      0.000      0.724      0.470      -0.000       0.000
acc                  -0.0989      0.102     -0.968      0.334      -0.300       0.102
cyl[T.6]:acc         -0.0065      0.018     -0.364      0.716      -0.042       0.029
cyl[T.8]:acc          0.0386      0.027      1.431      0.153      -0.014       0.092
year                 -0.0033      0.023     -0.142      0.887      -0.048       0.042
cyl[T.6]:year        -0.0151      0.008     -1.856      0.064      -0.031       0.001
cyl[T.8]:year        -0.0245      0.016     -1.553      0.121      -0.056       0.007
domestic              1.1169      0.447      2.499      0.013       0.238       1.996
cyl[T.6]:domestic    -0.0061      0.084     -0.072      0.942      -0.172       0.160
cyl[T.8]:domestic     0.6175      0.703      0.878      0.380      -0.765       2.001
disp_sq             7.17e-06   9.67e-06      0.741      0.459   -1.19e-05    2.62e-05
hp_sq             -4.364e-05   2.41e-05     -1.812      0.071    -9.1e-05    3.72e-06
wt_sq             -4.155e-08   4.27e-08     -0.974      0.331   -1.25e-07    4.24e-08
acc_sq            -7.326e-06      0.001     -0.005      0.996      -0.003       0.003
disp:hp            1.023e-05   1.64e-05      0.624      0.533    -2.2e-05    4.25e-05
disp:wt            1.277e-07   9.71e-07      0.131      0.896   -1.78e-06    2.04e-06
disp:acc           8.156e-05      0.000      0.526      0.599      -0.000       0.000
disp:year             0.0002   9.55e-05      1.995      0.047    2.73e-06       0.000
disp:domestic        -0.0007      0.002     -0.429      0.668      -0.004       0.002
hp:wt              1.111e-06   1.79e-06      0.620      0.535   -2.41e-06    4.63e-06
hp:acc               -0.0007      0.000     -2.217      0.027      -0.001   -8.17e-05
hp:year              -0.0003      0.000     -1.758      0.080      -0.001    3.62e-05
hp:domestic          -0.0003      0.002     -0.131      0.896      -0.004       0.004
wt:acc             8.498e-06   1.39e-05      0.613      0.540   -1.88e-05    3.58e-05
wt:year             5.67e-06   8.68e-06      0.653      0.514   -1.14e-05    2.27e-05
wt:domestic        5.578e-05      0.000      0.544      0.587      -0.000       0.000
acc:year              0.0017      0.001      1.575      0.116      -0.000       0.004
acc:domestic         -0.0265      0.009     -2.790      0.006      -0.045      -0.008
year:domestic        -0.0094      0.005     -1.902      0.058      -0.019       0.000
==============================================================================
Omnibus:                        9.669   Durbin-Watson:                   1.645
Prob(Omnibus):                  0.008   Jarque-Bera (JB):               16.986
Skew:                           0.065   Prob(JB):                     0.000205
Kurtosis:                       4.023   Cond. No.                     1.15e+16
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.58e-16. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</pre></div>
</div>
</div>
</div>
<p>Creemos que es bastante improbable que realmente necesitemos todos estos términos. Hay bastantes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display number of coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>40
</pre></div>
</div>
</div>
</div>
<p>A seguir, vamos a utilizar la selección de modelos para reducir este modelo. Diferente de la función  que hemos propuesto encima, aquí se debe especificar el modelo menor y el modelo mayor que se quiere probar. En este caso, el modelo menor es el modelo sin ningún término de orden superior, es decir, el modelo lineal. El modelo mayor es el modelo que acabamos de ajustar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>

<span class="k">class</span> <span class="nc">SMWrapper2</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">big_model</span><span class="p">,</span> <span class="n">small_model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">big_model</span> <span class="o">=</span> <span class="n">big_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">small_model</span> <span class="o">=</span> <span class="n">small_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_attrs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;aic&#39;</span><span class="p">:</span> <span class="s1">&#39;evaluate_aic&#39;</span><span class="p">,</span>
            <span class="s1">&#39;bic&#39;</span><span class="p">:</span> <span class="s1">&#39;evaluate_bic&#39;</span><span class="p">,</span>
            <span class="s1">&#39;mse&#39;</span><span class="p">:</span> <span class="s1">&#39;evaluate_mse&#39;</span>
        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Note: This is a dummy method, as the model is already fit outside the class</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Use the &#39;predict&#39; method of the underlying statsmodels model</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">evaluate_aic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">endog</span><span class="p">,</span> <span class="n">exog</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="p">,</span> <span class="n">exog</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">aic</span>
    
    <span class="k">def</span> <span class="nf">evaluate_bic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">endog</span><span class="p">,</span> <span class="n">exog</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="p">,</span> <span class="n">exog</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bic</span>
    
    <span class="k">def</span> <span class="nf">evaluate_mse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">mse</span>

<span class="k">def</span> <span class="nf">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="p">,</span> <span class="n">small_model</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">big_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">big_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog</span>

    <span class="n">sm_model</span> <span class="o">=</span> <span class="n">SMWrapper2</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">big_model</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">small_model</span><span class="p">)</span>
    <span class="n">num_features_small_model</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">small_model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">num_features_big_model</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">big_model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;aic&#39;</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">sm_model</span><span class="o">.</span><span class="n">evaluate_aic</span>
    <span class="k">elif</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;bic&#39;</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">sm_model</span><span class="o">.</span><span class="n">evaluate_bic</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid criterion. Choose &#39;aic&#39; or &#39;bic&#39;&quot;</span><span class="p">)</span>

    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">sm_model</span><span class="p">,</span> 
              <span class="n">k_features</span><span class="o">=</span><span class="p">(</span><span class="n">num_features_small_model</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_features_big_model</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> 
              <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span> 
              <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
              <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span>   
              <span class="n">cv</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sfs</span>

<span class="c1"># Define the small model</span>
<span class="n">autompg_small_mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;np.log(mpg) ~ 1&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">autompg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">sfs_result</span> <span class="o">=</span> <span class="n">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">autompg_small_mod</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sfs_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">sfs_result</span><span class="o">.</span><span class="n">get_metric_dict</span><span class="p">())</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sfs_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the small model</span>
<span class="n">autompg_small_mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;np.log(mpg) ~ 1&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">autompg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># This part assumes that the function &#39;select_features_using_models&#39; is correctly defined elsewhere in your code</span>
<span class="n">sfs_result</span> <span class="o">=</span> <span class="n">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">autompg_small_mod</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sfs_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">sfs_result</span><span class="o">.</span><span class="n">get_metric_dict</span><span class="p">())</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sfs_results</span><span class="p">)</span>

<span class="c1"># Determine the optimal subset of features</span>
<span class="n">best_feature_idx</span> <span class="o">=</span> <span class="n">sfs_result</span><span class="o">.</span><span class="n">k_feature_idx_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create DataFrames from arrays (assuming X_big, y_big, and columns are correctly defined elsewhere in your code)</span>
<span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;response&#39;</span><span class="p">)</span>

<span class="c1"># Extract the optimal features.</span>
<span class="n">best_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_big_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">best_feature_idx</span><span class="p">]</span>
<span class="n">X_optimal_df</span> <span class="o">=</span> <span class="n">X_big_df</span><span class="p">[</span><span class="n">best_feature_names</span><span class="p">]</span>

<span class="c1"># Fit the model</span>
<span class="n">optimal_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_big_df</span><span class="p">,</span> <span class="n">X_optimal_df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">optimal_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                          feature_idx             cv_scores  \
1                                                (9,)  [315.69912748970955]   
2                                             (9, 15)   [543.3015193542258]   
3                                         (9, 15, 23)   [580.2073106351925]   
4                                     (9, 15, 23, 31)   [597.1823335937511]   
5                                 (9, 15, 23, 29, 31)   [608.9545173593897]   
6                             (9, 15, 23, 24, 29, 31)   [614.7656873727124]   
7                          (6, 9, 15, 23, 24, 29, 31)   [621.4372952765893]   
8                      (6, 9, 13, 15, 23, 24, 29, 31)   [623.1970272187843]   
9                   (6, 8, 9, 13, 15, 23, 24, 29, 31)   [625.6653788323961]   
10              (6, 8, 9, 13, 14, 15, 23, 24, 29, 31)   [629.4667596303166]   
11          (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32)   [635.4514184726497]   
12      (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35)   [641.1444320704597]   
13  (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35, 38)   [643.6424065240669]   
14  (6, 8, 9, 13, 14, 15, 18, 23, 24, 29, 31, 32, ...   [647.1488272814236]   
15  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...   [648.2629885809101]   
16  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...   [649.1395202852952]   
17  (6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 29, ...   [651.7846839652168]   
18  (0, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 2...   [651.7846839652168]   
19  (0, 2, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24...   [651.7846839652168]   
20  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 23...   [651.3104063603168]   
21  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...    [651.340027993715]   
22  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...   [651.5074377835997]   
23  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...   [651.9083313728661]   
24  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...    [650.966531155703]   
25  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...   [650.2483177861106]   
26  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...   [654.1148231141331]   
27  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...   [653.3112754314348]   
28  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...     [651.99408135762]   
29  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...   [650.4856170614416]   
30  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...   [649.0824787753393]   
31  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...   [647.7809777978016]   
32  (0, 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 1...   [647.8281415286049]   
33  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...   [646.1836491035597]   
34  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...    [644.343068150745]   
35  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...   [642.4577273661946]   
36  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...   [640.7976235787478]   
37  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...   [639.0550188715988]   
38  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...    [637.298589879453]   
39  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   [635.3856930115237]   
40  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   [633.4047020598741]   

     avg_score                                      feature_names ci_bound  \
1   315.699127                                               (9,)      NaN   
2   543.301519                                            (9, 15)      NaN   
3   580.207311                                        (9, 15, 23)      NaN   
4   597.182334                                    (9, 15, 23, 31)      NaN   
5   608.954517                                (9, 15, 23, 29, 31)      NaN   
6   614.765687                            (9, 15, 23, 24, 29, 31)      NaN   
7   621.437295                         (6, 9, 15, 23, 24, 29, 31)      NaN   
8   623.197027                     (6, 9, 13, 15, 23, 24, 29, 31)      NaN   
9   625.665379                  (6, 8, 9, 13, 15, 23, 24, 29, 31)      NaN   
10   629.46676              (6, 8, 9, 13, 14, 15, 23, 24, 29, 31)      NaN   
11  635.451418          (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32)      NaN   
12  641.144432      (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35)      NaN   
13  643.642407  (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35, 38)      NaN   
14  647.148827  (6, 8, 9, 13, 14, 15, 18, 23, 24, 29, 31, 32, ...      NaN   
15  648.262989  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...      NaN   
16   649.13952  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...      NaN   
17  651.784684  (6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 29, ...      NaN   
18  651.784684  (0, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 2...      NaN   
19  651.784684  (0, 2, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24...      NaN   
20  651.310406  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 23...      NaN   
21  651.340028  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...      NaN   
22  651.507438  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...      NaN   
23  651.908331  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...      NaN   
24  650.966531  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...      NaN   
25  650.248318  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
26  654.114823  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
27  653.311275  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
28  651.994081  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
29  650.485617  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...      NaN   
30  649.082479  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...      NaN   
31  647.780978  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...      NaN   
32  647.828142  (0, 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 1...      NaN   
33  646.183649  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
34  644.343068  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
35  642.457727  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
36  640.797624  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
37  639.055019  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...      NaN   
38   637.29859  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...      NaN   
39  635.385693  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      NaN   
40  633.404702  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      NaN   

   std_dev std_err  
1      0.0     NaN  
2      0.0     NaN  
3      0.0     NaN  
4      0.0     NaN  
5      0.0     NaN  
6      0.0     NaN  
7      0.0     NaN  
8      0.0     NaN  
9      0.0     NaN  
10     0.0     NaN  
11     0.0     NaN  
12     0.0     NaN  
13     0.0     NaN  
14     0.0     NaN  
15     0.0     NaN  
16     0.0     NaN  
17     0.0     NaN  
18     0.0     NaN  
19     0.0     NaN  
20     0.0     NaN  
21     0.0     NaN  
22     0.0     NaN  
23     0.0     NaN  
24     0.0     NaN  
25     0.0     NaN  
26     0.0     NaN  
27     0.0     NaN  
28     0.0     NaN  
29     0.0     NaN  
30     0.0     NaN  
31     0.0     NaN  
32     0.0     NaN  
33     0.0     NaN  
34     0.0     NaN  
35     0.0     NaN  
36     0.0     NaN  
37     0.0     NaN  
38     0.0     NaN  
39     0.0     NaN  
40     0.0     NaN  
Best features by index: (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20, 22, 23, 24, 26, 29, 30, 31, 32, 35, 37, 38, 39)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_methods.py:262: RuntimeWarning: Degrees of freedom &lt;= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
c:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="ne">c</span>:\Users\a.tabaresp\MEL_202302\Week_10\10-1-Selection.ipynb Celda 101 line 1
     <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y230sZmlsZQ%3D%3D?line=11&#39;</span><span class="o">&gt;</span><span class="mi">12</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
     <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y230sZmlsZQ%3D%3D?line=13&#39;</span><span class="o">&gt;</span><span class="mi">14</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="c1"># Create DataFrames from arrays (assuming X_big, y_big, and columns are correctly defined elsewhere in your code)</span>
<span class="o">---&gt;</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y230sZmlsZQ%3D%3D?line=14&#39;</span><span class="o">&gt;</span><span class="mi">15</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
     <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y230sZmlsZQ%3D%3D?line=15&#39;</span><span class="o">&gt;</span><span class="mi">16</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;response&#39;</span><span class="p">)</span>
     <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y230sZmlsZQ%3D%3D?line=17&#39;</span><span class="o">&gt;</span><span class="mi">18</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="c1"># Extract the optimal features.</span>

<span class="ne">NameError</span>: name &#39;columns&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the small model</span>
<span class="n">autompg_small_mod</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;np.log(mpg) ~ 1&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">autompg</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">sfs_result</span> <span class="o">=</span> <span class="n">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">autompg_small_mod</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sfs_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">sfs_result</span><span class="o">.</span><span class="n">get_metric_dict</span><span class="p">())</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sfs_results</span><span class="p">)</span>

<span class="c1"># Determina el subconjunto de características óptimo</span>
<span class="n">best_feature_idx</span> <span class="o">=</span> <span class="n">sfs_result</span><span class="o">.</span><span class="n">k_feature_idx_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>



<span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">)</span>


<span class="c1"># Extraer las características óptimas.</span>
<span class="n">best_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_big_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">best_feature_idx</span><span class="p">]</span>
<span class="n">X_optimal_df</span> <span class="o">=</span> <span class="n">X_big_df</span><span class="p">[</span><span class="n">best_feature_names</span><span class="p">]</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">optimal_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_big_df</span><span class="p">,</span> <span class="n">X_optimal_df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">optimal_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                          feature_idx             cv_scores  \
1                                                (9,)  [315.69912748970955]   
2                                             (9, 15)   [543.3015193542258]   
3                                         (9, 15, 23)   [580.2073106351925]   
4                                     (9, 15, 23, 31)   [597.1823335937511]   
5                                 (9, 15, 23, 29, 31)   [608.9545173593897]   
6                             (9, 15, 23, 24, 29, 31)   [614.7656873727124]   
7                          (6, 9, 15, 23, 24, 29, 31)   [621.4372952765893]   
8                      (6, 9, 13, 15, 23, 24, 29, 31)   [623.1970272187843]   
9                   (6, 8, 9, 13, 15, 23, 24, 29, 31)   [625.6653788323961]   
10              (6, 8, 9, 13, 14, 15, 23, 24, 29, 31)   [629.4667596303166]   
11          (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32)   [635.4514184726497]   
12      (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35)   [641.1444320704597]   
13  (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35, 38)   [643.6424065240669]   
14  (6, 8, 9, 13, 14, 15, 18, 23, 24, 29, 31, 32, ...   [647.1488272814236]   
15  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...   [648.2629885809101]   
16  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...   [649.1395202852952]   
17  (6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 29, ...   [651.7846839652168]   
18  (0, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 2...   [651.7846839652168]   
19  (0, 2, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24...   [651.7846839652168]   
20  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 23...   [651.3104063603168]   
21  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...    [651.340027993715]   
22  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...   [651.5074377835997]   
23  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...   [651.9083313728661]   
24  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...    [650.966531155703]   
25  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...   [650.2483177861106]   
26  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...   [654.1148231141331]   
27  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...   [653.3112754314348]   
28  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...     [651.99408135762]   
29  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...   [650.4856170614416]   
30  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...   [649.0824787753393]   
31  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...   [647.7809777978016]   
32  (0, 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 1...   [647.8281415286049]   
33  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...   [646.1836491035597]   
34  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...    [644.343068150745]   
35  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...   [642.4577273661946]   
36  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...   [640.7976235787478]   
37  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...   [639.0550188715988]   
38  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...    [637.298589879453]   
39  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   [635.3856930115237]   
40  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   [633.4047020598741]   

     avg_score                                      feature_names ci_bound  \
1   315.699127                                               (9,)      NaN   
2   543.301519                                            (9, 15)      NaN   
3   580.207311                                        (9, 15, 23)      NaN   
4   597.182334                                    (9, 15, 23, 31)      NaN   
5   608.954517                                (9, 15, 23, 29, 31)      NaN   
6   614.765687                            (9, 15, 23, 24, 29, 31)      NaN   
7   621.437295                         (6, 9, 15, 23, 24, 29, 31)      NaN   
8   623.197027                     (6, 9, 13, 15, 23, 24, 29, 31)      NaN   
9   625.665379                  (6, 8, 9, 13, 15, 23, 24, 29, 31)      NaN   
10   629.46676              (6, 8, 9, 13, 14, 15, 23, 24, 29, 31)      NaN   
11  635.451418          (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32)      NaN   
12  641.144432      (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35)      NaN   
13  643.642407  (6, 8, 9, 13, 14, 15, 23, 24, 29, 31, 32, 35, 38)      NaN   
14  647.148827  (6, 8, 9, 13, 14, 15, 18, 23, 24, 29, 31, 32, ...      NaN   
15  648.262989  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...      NaN   
16   649.13952  (6, 8, 9, 13, 14, 15, 18, 20, 23, 24, 29, 31, ...      NaN   
17  651.784684  (6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 29, ...      NaN   
18  651.784684  (0, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24, 2...      NaN   
19  651.784684  (0, 2, 6, 8, 9, 12, 13, 14, 15, 18, 20, 23, 24...      NaN   
20  651.310406  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 23...      NaN   
21  651.340028  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...      NaN   
22  651.507438  (0, 2, 6, 8, 9, 12, 13, 14, 15, 16, 18, 20, 22...      NaN   
23  651.908331  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...      NaN   
24  650.966531  (0, 2, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20...      NaN   
25  650.248318  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
26  654.114823  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
27  653.311275  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
28  651.994081  (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18,...      NaN   
29  650.485617  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...      NaN   
30  649.082479  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...      NaN   
31  647.780978  (0, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, ...      NaN   
32  647.828142  (0, 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 1...      NaN   
33  646.183649  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
34  644.343068  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
35  642.457727  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
36  640.797624  (0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 1...      NaN   
37  639.055019  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...      NaN   
38   637.29859  (0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14...      NaN   
39  635.385693  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      NaN   
40  633.404702  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      NaN   

   std_dev std_err  
1      0.0     NaN  
2      0.0     NaN  
3      0.0     NaN  
4      0.0     NaN  
5      0.0     NaN  
6      0.0     NaN  
7      0.0     NaN  
8      0.0     NaN  
9      0.0     NaN  
10     0.0     NaN  
11     0.0     NaN  
12     0.0     NaN  
13     0.0     NaN  
14     0.0     NaN  
15     0.0     NaN  
16     0.0     NaN  
17     0.0     NaN  
18     0.0     NaN  
19     0.0     NaN  
20     0.0     NaN  
21     0.0     NaN  
22     0.0     NaN  
23     0.0     NaN  
24     0.0     NaN  
25     0.0     NaN  
26     0.0     NaN  
27     0.0     NaN  
28     0.0     NaN  
29     0.0     NaN  
30     0.0     NaN  
31     0.0     NaN  
32     0.0     NaN  
33     0.0     NaN  
34     0.0     NaN  
35     0.0     NaN  
36     0.0     NaN  
37     0.0     NaN  
38     0.0     NaN  
39     0.0     NaN  
40     0.0     NaN  
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_methods.py:262: RuntimeWarning: Degrees of freedom &lt;= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
c:\ProgramData\Anaconda3\lib\site-packages\numpy\core\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Determina el subconjunto de características óptimo</span>
<span class="n">best_feature_idx</span> <span class="o">=</span> <span class="n">sfs_result</span><span class="o">.</span><span class="n">k_feature_idx_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>



<span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">)</span>


<span class="c1"># Extraer las características óptimas.</span>
<span class="n">best_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_big_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">best_feature_idx</span><span class="p">]</span>
<span class="n">X_optimal_df</span> <span class="o">=</span> <span class="n">X_big_df</span><span class="p">[</span><span class="n">best_feature_names</span><span class="p">]</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">optimal_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_big_df</span><span class="p">,</span> <span class="n">X_optimal_df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">optimal_model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best features by index: (0, 2, 3, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 20, 22, 23, 24, 26, 29, 30, 31, 32, 35, 37, 38, 39)
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="ne">c</span>:\Users\a.tabaresp\MEL_202302\Week_10\10-1-Selection.ipynb Celda 101 line 7
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y202sZmlsZQ%3D%3D?line=1&#39;</span><span class="o">&gt;</span><span class="mi">2</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">best_feature_idx</span> <span class="o">=</span> <span class="n">sfs_result</span><span class="o">.</span><span class="n">k_feature_idx_</span>
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y202sZmlsZQ%3D%3D?line=2&#39;</span><span class="o">&gt;</span><span class="mi">3</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="o">----&gt;</span> <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y202sZmlsZQ%3D%3D?line=6&#39;</span><span class="o">&gt;</span><span class="mi">7</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
      <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y202sZmlsZQ%3D%3D?line=7&#39;</span><span class="o">&gt;</span><span class="mi">8</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">)</span>
     <span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s1">&#39;vscode-notebook-cell:/c%3A/Users/a.tabaresp/MEL_202302/Week_10/10-1-Selection.ipynb#Y202sZmlsZQ%3D%3D?line=10&#39;</span><span class="o">&gt;</span><span class="mi">11</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="c1"># Extraer las características óptimas.</span>

<span class="ne">NameError</span>: name &#39;columns&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>Vamos a tratar de búsqueda hacia atrás con tanto <span class="math notranslate nohighlight">\(\text{AIC}\)</span> y <span class="math notranslate nohighlight">\(\text{BIC}\)</span> para tratar de encontrar un modelo más pequeño, más razonable.</p>
<p>También utilizamos <span class="math notranslate nohighlight">\(\text{BIC}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sfs_result_bic</span> <span class="o">=</span> <span class="n">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">autompg_small_mod</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="c1"># Determina el subconjunto de características óptimo</span>
<span class="n">best_feature_idx_bic</span><span class="o">=</span> <span class="n">sfs_result_bic</span><span class="o">.</span><span class="n">k_feature_idx_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx_bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">X_big</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># Asumiendo que &#39;const&#39; es el que sobra, lo eliminamos de la lista</span>
    <span class="k">if</span> <span class="s1">&#39;const&#39;</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
        <span class="n">columns</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;const&#39;</span><span class="p">)</span>

<span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">)</span>


<span class="c1"># Extraer las características óptimas.</span>
<span class="n">best_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_big_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">best_feature_idx_bic</span><span class="p">]</span>
<span class="n">X_optimal_df_bic</span> <span class="o">=</span> <span class="n">X_big_df</span><span class="p">[</span><span class="n">best_feature_names</span><span class="p">]</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">optimal_model_bic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_big_df</span><span class="p">,</span> <span class="n">X_optimal_df_bic</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">optimal_model_bic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best features by index: (6, 9, 15, 23, 24, 29, 31)
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:            np.log(mpg)   R-squared (uncentered):                   0.998
Model:                            OLS   Adj. R-squared (uncentered):              0.998
Method:                 Least Squares   F-statistic:                          3.313e+04
Date:                Mon, 09 Oct 2023   Prob (F-statistic):                        0.00
Time:                        01:28:18   Log-Likelihood:                          251.94
No. Observations:                 383   AIC:                                     -489.9
Df Residuals:                     376   BIC:                                     -462.3
Df Model:                           7                                                  
Covariance Type:            nonrobust                                                  
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
hp                0.0050      0.001      8.013      0.000       0.004       0.006
wt               -0.0002    6.1e-05     -2.787      0.006      -0.000      -5e-05
year              0.0470      0.001     39.451      0.000       0.045       0.049
wt_sq         -2.493e-09    8.7e-09     -0.286      0.775   -1.96e-08    1.46e-08
acc_sq            0.0011      0.000      7.441      0.000       0.001       0.001
disp:domestic    -0.0005      0.000     -5.009      0.000      -0.001      -0.000
hp:acc           -0.0004   5.37e-05     -7.869      0.000      -0.001      -0.000
==============================================================================
Omnibus:                        5.207   Durbin-Watson:                   1.246
Prob(Omnibus):                  0.074   Jarque-Bera (JB):                5.425
Skew:                          -0.191   Prob(JB):                       0.0664
Kurtosis:                       3.440   Cond. No.                     2.09e+06
==============================================================================

Notes:
[1] R² is computed without centering (uncentered) since the model does not contain a constant.
[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[3] The condition number is large, 2.09e+06. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Si observamos los coeficientes de los dos modelos elegidos, vemos que siguen siendo bastante grandes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># number of coefficients in the full model</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">optimal_model</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">optimal_model_bic</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>40
26
7
</pre></div>
</div>
</div>
</div>
<p>Aunque los modelos seleccionados son mucho más pequeños que el modelo completo original. Obsérvese  que los modelos resultantes NO respetan la jerarquía!!! Vemos que hay tambien problemas de variables con multicolinealidad.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>

<span class="k">def</span> <span class="nf">has_higher_order_term</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">feature</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks if there is a higher order term present in the columns for a given feature.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
        <span class="c1"># Ensuring col is a string</span>
        <span class="n">col</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
        <span class="c1"># Check if feature name is part of a higher order term name</span>
        <span class="k">if</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;^2&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;:&#39;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="k">def</span> <span class="nf">is_higher_order_or_interaction</span><span class="p">(</span><span class="n">feature_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;^&#39;</span> <span class="ow">in</span> <span class="n">feature_name</span> <span class="ow">or</span> <span class="s1">&#39;:&#39;</span> <span class="ow">in</span> <span class="n">feature_name</span>


<span class="k">def</span> <span class="nf">get_main_effects</span><span class="p">(</span><span class="n">feature_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;:&#39;</span> <span class="ow">in</span> <span class="n">feature_name</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">feature_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s1">&#39;^&#39;</span> <span class="ow">in</span> <span class="n">feature_name</span><span class="p">:</span>
        <span class="n">base</span><span class="p">,</span> <span class="n">power</span> <span class="o">=</span> <span class="n">feature_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;^&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">base</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">adjust_for_hierarchy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">original_X</span><span class="p">):</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_higher_order_or_interaction</span><span class="p">(</span><span class="n">feature</span><span class="p">):</span>
            <span class="n">main_effects</span> <span class="o">=</span> <span class="n">get_main_effects</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">effect</span> <span class="ow">in</span> <span class="n">main_effects</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">effect</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
                    <span class="n">X</span><span class="p">[</span><span class="n">effect</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_X</span><span class="p">[</span><span class="n">effect</span><span class="p">]</span>
                    <span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">effect</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_higher_order_term</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">feature</span><span class="p">):</span>
            <span class="n">X</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_X</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">SMWrapper3</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">big_model</span><span class="p">,</span> <span class="n">small_model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">big_model</span> <span class="o">=</span> <span class="n">big_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">small_model</span> <span class="o">=</span> <span class="n">small_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_attrs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;evaluate_aic&#39;</span><span class="p">:</span> <span class="s1">&#39;evaluate_aic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;evaluate_bic&#39;</span><span class="p">:</span> <span class="s1">&#39;evaluate_bic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;evaluate_mse&#39;</span><span class="p">:</span> <span class="s1">&#39;evaluate_mse&#39;</span>
<span class="p">}</span>

    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Note: This is a dummy method, as the model is already fit outside the class</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Use the &#39;predict&#39; method of the underlying statsmodels model</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">evaluate_aic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="c1"># Ensure column names are strings</span>
        <span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">adjust_for_hierarchy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">selected_features_</span> <span class="o">=</span> <span class="n">columns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">columns</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">aic</span>

    <span class="k">def</span> <span class="nf">evaluate_bic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="c1"># Ensure column names are strings</span>
        <span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

        <span class="n">columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">adjust_for_hierarchy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">selected_features_</span> <span class="o">=</span> <span class="n">columns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">columns</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bic</span>

    <span class="k">def</span> <span class="nf">evaluate_mse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="p">((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">mse</span>

<span class="k">def</span> <span class="nf">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="p">,</span> <span class="n">small_model</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">big_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">exog</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">big_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog</span>

    <span class="n">sm_model</span> <span class="o">=</span> <span class="n">SMWrapper3</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">big_model</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">small_model</span><span class="p">)</span>
    <span class="n">num_features_small_model</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">small_model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">num_features_big_model</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">big_model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>


    
    <span class="c1"># Define the scorer based on the selected criterion</span>
    <span class="k">if</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;aic&#39;</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">sm_model</span><span class="o">.</span><span class="n">evaluate_aic</span>
    <span class="k">elif</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">&#39;bic&#39;</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">sm_model</span><span class="o">.</span><span class="n">evaluate_bic</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid criterion. Choose &#39;aic&#39; or &#39;bic&#39;&quot;</span><span class="p">)</span>


    <span class="c1"># Define and fit SFS with the full feature set</span>
    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">sm_model</span><span class="p">,</span> 
              <span class="n">k_features</span><span class="o">=</span><span class="p">(</span><span class="n">num_features_small_model</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_features_big_model</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> 
              <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span> 
              <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
              <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span>   
              <span class="n">cv</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Update X to keep only the selected features</span>
    <span class="n">X_selected</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="nb">list</span><span class="p">(</span><span class="n">sfs</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
    
    <span class="c1"># Setup SFS with updated k_features for X_selected</span>
    <span class="n">sfs</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">sm_model</span><span class="p">,</span> 
              <span class="n">k_features</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_selected</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> 
              <span class="n">forward</span><span class="o">=</span><span class="n">forward</span><span class="p">,</span> 
              <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
              <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span>   
              <span class="n">cv</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">sfs</span> <span class="o">=</span> <span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_selected</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">sfs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sfs_result_bic</span> <span class="o">=</span> <span class="n">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">autompg_small_mod</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;aic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="c1"># Determina el subconjunto de características óptimo</span>
<span class="n">best_feature_idx_bic</span><span class="o">=</span> <span class="n">sfs_result_bic</span><span class="o">.</span><span class="n">k_feature_idx_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx_bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">X_big</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># Asumiendo que &#39;const&#39; es el que sobra, lo eliminamos de la lista</span>
    <span class="k">if</span> <span class="s1">&#39;const&#39;</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
        <span class="n">columns</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;const&#39;</span><span class="p">)</span>

<span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">)</span>


<span class="c1"># Extraer las características óptimas.</span>
<span class="n">best_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_big_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">best_feature_idx_bic</span><span class="p">]</span>
<span class="n">X_optimal_df_bic</span> <span class="o">=</span> <span class="n">X_big_df</span><span class="p">[</span><span class="n">best_feature_names</span><span class="p">]</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">autompg_mod_back_aic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_big_df</span><span class="p">,</span> <span class="n">X_optimal_df_bic</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">autompg_mod_back_aic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best features by index: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:            np.log(mpg)   R-squared:                       0.912
Model:                            OLS   Adj. R-squared:                  0.906
Method:                 Least Squares   F-statistic:                     154.2
Date:                Mon, 09 Oct 2023   Prob (F-statistic):          1.40e-172
Time:                        08:23:47   Log-Likelihood:                 332.74
No. Observations:                 383   AIC:                            -615.5
Df Residuals:                     358   BIC:                            -516.8
Df Model:                          24                                         
Covariance Type:            nonrobust                                         
=====================================================================================
                        coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept             2.5365      0.351      7.217      0.000       1.845       3.228
cyl[T.6]              0.3143      0.425      0.740      0.460      -0.521       1.149
cyl[T.8]             -0.0836      0.343     -0.244      0.808      -0.759       0.591
disp                 -0.0029      0.001     -1.983      0.048      -0.006    -2.4e-05
cyl[T.6]:disp         0.0004      0.002      0.231      0.818      -0.003       0.004
cyl[T.8]:disp         0.0003      0.003      0.102      0.919      -0.006       0.006
hp                   -0.0035      0.002     -1.425      0.155      -0.008       0.001
cyl[T.6]:hp          -0.0021      0.002     -0.983      0.326      -0.006       0.002
cyl[T.8]:hp          -0.0039      0.003     -1.196      0.233      -0.010       0.003
wt                 1.166e-05      0.000      0.064      0.949      -0.000       0.000
cyl[T.6]:wt           0.0001   9.71e-05      1.273      0.204   -6.74e-05       0.000
cyl[T.8]:wt           0.0002      0.000      1.750      0.081   -2.75e-05       0.000
acc                  -0.1009      0.027     -3.787      0.000      -0.153      -0.049
cyl[T.6]:acc         -0.0158      0.012     -1.331      0.184      -0.039       0.008
cyl[T.8]:acc          0.0144      0.011      1.330      0.184      -0.007       0.036
year                  0.0313      0.002     13.921      0.000       0.027       0.036
cyl[T.6]:year        -0.0043      0.005     -0.951      0.342      -0.013       0.005
cyl[T.8]:year        -0.0052      0.005     -1.011      0.313      -0.015       0.005
domestic             -0.0303      0.020     -1.549      0.122      -0.069       0.008
cyl[T.6]:domestic     0.0386      0.067      0.573      0.567      -0.094       0.171
cyl[T.8]:domestic    -0.0836      0.343     -0.244      0.808      -0.759       0.591
disp_sq            4.036e-07   7.87e-06      0.051      0.959   -1.51e-05    1.59e-05
hp_sq             -5.965e-06   1.53e-05     -0.391      0.696    -3.6e-05    2.41e-05
wt_sq             -4.379e-08    3.5e-08     -1.252      0.211   -1.13e-07     2.5e-08
acc_sq                0.0028      0.001      3.669      0.000       0.001       0.004
disp:hp             1.88e-05   1.39e-05      1.348      0.179   -8.63e-06    4.62e-05
==============================================================================
Omnibus:                       14.381   Durbin-Watson:                   1.551
Prob(Omnibus):                  0.001   Jarque-Bera (JB):               28.976
Skew:                           0.147   Prob(JB):                     5.10e-07
Kurtosis:                       4.315   Cond. No.                     1.15e+16
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.56e-16. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sfs_result_bic</span> <span class="o">=</span> <span class="n">select_features_using_models</span><span class="p">(</span><span class="n">big_model</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="p">,</span> <span class="n">small_model</span><span class="o">=</span><span class="n">autompg_small_mod</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="c1"># Determina el subconjunto de características óptimo</span>
<span class="n">best_feature_idx_bic</span><span class="o">=</span> <span class="n">sfs_result_bic</span><span class="o">.</span><span class="n">k_feature_idx_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best features by index: </span><span class="si">{</span><span class="n">best_feature_idx_bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">X_big</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># Asumiendo que &#39;const&#39; es el que sobra, lo eliminamos de la lista</span>
    <span class="k">if</span> <span class="s1">&#39;const&#39;</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
        <span class="n">columns</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;const&#39;</span><span class="p">)</span>

<span class="n">X_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_big</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">y_big_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_big</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">autompg_big_mod</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">endog_names</span><span class="p">)</span>


<span class="c1"># Extraer las características óptimas.</span>
<span class="n">best_feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_big_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">best_feature_idx_bic</span><span class="p">]</span>
<span class="n">X_optimal_df_bic</span> <span class="o">=</span> <span class="n">X_big_df</span><span class="p">[</span><span class="n">best_feature_names</span><span class="p">]</span>

<span class="c1"># Ajustar el modelo</span>
<span class="n">autompg_mod_back_bic</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y_big_df</span><span class="p">,</span> <span class="n">X_optimal_df_bic</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">autompg_mod_back_bic</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best features by index: (0, 1, 2, 3, 4, 5, 6)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:            np.log(mpg)   R-squared:                       0.803
Model:                            OLS   Adj. R-squared:                  0.800
Method:                 Least Squares   F-statistic:                     256.0
Date:                Mon, 09 Oct 2023   Prob (F-statistic):          1.89e-129
Time:                        08:24:00   Log-Likelihood:                 179.22
No. Observations:                 383   AIC:                            -344.4
Df Residuals:                     376   BIC:                            -316.8
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept         3.9786      0.060     66.826      0.000       3.862       4.096
cyl[T.6]          0.0582      0.139      0.419      0.675      -0.215       0.331
cyl[T.8]         -0.8246      0.128     -6.451      0.000      -1.076      -0.573
disp             -0.0024      0.001     -4.293      0.000      -0.004      -0.001
cyl[T.6]:disp    -0.0003      0.001     -0.385      0.701      -0.002       0.001
cyl[T.8]:disp     0.0032      0.001      5.220      0.000       0.002       0.004
hp               -0.0045      0.001     -7.968      0.000      -0.006      -0.003
==============================================================================
Omnibus:                       15.619   Durbin-Watson:                   0.981
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               20.149
Skew:                           0.362   Prob(JB):                     4.21e-05
Kurtosis:                       3.859   Cond. No.                     5.51e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 5.51e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre></div>
</div>
</div>
</div>
<p>Calculando el LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> para cada uno, vemos que el modelo elegido utilizando <span class="math notranslate nohighlight">\(\text{BIC}\)</span> es el que mejor funciona. Esto significa que es a la vez el mejor modelo de predicción, ya que consigue el mejor LOOCV pero también el mejor modelo para la explicación, ya que también es el más pequeño. el más pequeño.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">autompg_big_mod</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.4605426586361027
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">autompg_mod_back_aic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.10160437691142798
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calc_loocv_rmse</span><span class="p">(</span><span class="n">autompg_mod_back_bic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.15499022129325363
</pre></div>
</div>
</div>
</div>
</section>
<section id="explicacion-frente-a-prediccion">
<h1>Explicación frente a predicción<a class="headerlink" href="#explicacion-frente-a-prediccion" title="Permalink to this heading">#</a></h1>
<p>A lo largo de este capítulo, hemos intentado encontrar modelos razonablemente “pequeños”, que sean buenos para <strong>explicar</strong> la relación entre la respuesta y los predictores, que también tengan errores pequeños y que, por tanto, sean buenos para hacer <strong>predicciones</strong>.</p>
<p>Discutiremos más a fondo el modelo <code class="docutils literal notranslate"><span class="pre">autompg_mod_back_aic</span></code> para explicar mejor la diferencia entre utilizar modelos para <em>explicar</em> y <em>predicar</em>. Este es el modelo ajustado a los datos <code class="docutils literal notranslate"><span class="pre">autompg</span></code> que se eligió utilizando la búsqueda hacia atrás y <span class="math notranslate nohighlight">\(\text{BIC}\)</span>, que obtuvo el LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span> más bajo de los modelos que consideramos.</p>
<p>Observe que se trata de un modelo algo “grande”, que utiliza parámetros 26, incluidos varios términos de interacción. ¿Nos importa que sea un modelo “grande”? La respuesta es, <strong>depende</strong>.</p>
<section id="explicacion">
<h2>Explicación<a class="headerlink" href="#explicacion" title="Permalink to this heading">#</a></h2>
<p>Supongamos que queremos utilizar este modelo como explicación. Tal vez seamos un fabricante de coches que intenta diseñar un vehículo que consuma menos combustible. En este caso, nos interesa saber qué variables predictoras son útiles para explicar la eficiencia del combustible del coche y cómo afectan esas variables a la eficiencia del combustible. Al comprender esta relación, podemos utilizar este conocimiento en nuestro beneficio a la hora de diseñar un coche.</p>
<p>Para explicar una relación, nos interesa que los modelos sean lo más pequeños posible, ya que los modelos más pequeños son fáciles de interpretar. Cuantos menos predictores, menos consideraciones tendremos que tener en cuenta en nuestro proceso de diseño. Además, cuantas menos interacciones y términos polinómicos haya, más fácil será interpretar cualquier parámetro, ya que las interpretaciones de los parámetros dependen de los parámetros que haya en el modelo.</p>
<p>Tenga en cuenta que los modelos <em>lineales</em> son bastante interpretables para empezar. Más adelante en su carrera de análisis de datos, verá modelos más complicados que pueden ajustarse mejor a los datos, pero son mucho más difíciles, si no imposibles de interpretar. Estos modelos no son muy útiles para explicar una relación.</p>
<p>Para encontrar modelos pequeños e interpretables, utilizaríamos criterios de selección que penalizan <em>explícitamente</em> los modelos más grandes, como el AIC y el BIC. En este caso seguimos obteniendo un modelo algo grande, pero mucho más pequeño que el modelo que utilizamos para iniciar el proceso de selección.</p>
</section>
<section id="prediccion">
<h2>Predicción<a class="headerlink" href="#prediccion" title="Permalink to this heading">#</a></h2>
<p>Supongamos que ahora, en lugar del fabricante que desea fabricar un coche, somos un consumidor que desea comprar un coche nuevo. Sin embargo, este coche en concreto es tan nuevo que no ha sido sometido a pruebas rigurosas, por lo que no estamos seguros de qué eficiencia de combustible podemos esperar. (Y, como escépticos que somos, no nos fiamos de lo que nos dice el fabricante).</p>
<p>En este caso, nos gustaría utilizar el modelo para ayudar a <em>predecir</em> la eficiencia de combustible de este coche basándonos en sus atributos, que son los predictores del modelo. Cuanto menores sean los errores del modelo, más confianza tendremos en su predicción. Así, para encontrar modelos de predicción, utilizaríamos criterios de selección que <em>implícitamente</em> penalizan los modelos más grandes, como LOOCV <span class="math notranslate nohighlight">\(\text{RMSE}\)</span>. Mientras el modelo no se ajuste en exceso, en realidad no nos importa lo grande que sea el modelo. Explicar la relación entre las variables no es nuestro objetivo aquí, ¡simplemente queremos saber qué tipo de eficiencia de combustible debemos esperar!</p>
<p>Si <strong>sólo</strong> nos importa la predicción, no tenemos que preocuparnos por la correlación frente a la causalidad ni por los supuestos del modelo.</p>
<p>Si una variable está correlacionada con la respuesta, en realidad no importa si causa un efecto en la respuesta, puede seguir siendo útil para la predicción. Por ejemplo, en niños de primaria, el número de calzado no <em>causa</em> que lean a un nivel más alto, pero podríamos utilizarlo para hacer una predicción sobre la capacidad lectora de un niño. Cuanto mayor sea su número de calzado, mejor leerán. Sin embargo, hay una variable al acecho: ¡su edad! (No envíe a sus hijos al colegio con zapatos de la talla 14, ¡no leerán mejor!).</p>
<p>Tampoco nos importan los supuestos del modelo. Los mínimos cuadrados son los mínimos cuadrados. Para un modelo especificado, encontrará los valores de los parámetros que minimizarán la pérdida de error al cuadrado. Sus resultados pueden ser en gran medida ininterpretables e inútiles para la inferencia, pero para la predicción nada de eso importa.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Week_10"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Lectura 10-1:  Selección de variables y creación de modelos</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#criterios-de-calidad">Criterios de calidad</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#criterio-de-informacion-de-akaike">Criterio de información de Akaike</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#criterio-de-informacion-bayesiano">Criterio de información bayesiano</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-cuadrado-ajustado">R-cuadrado ajustado</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rmse-de-validacion-cruzada">RMSE de validación cruzada</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#procedimientos-de-seleccion">Procedimientos de selección</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-hacia-atras">Búsqueda hacia atrás</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-hacia-delante">Búsqueda hacia delante</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-por-pasos">Búsqueda por pasos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#busqueda-exhaustiva">Búsqueda exhaustiva</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#terminos-de-orden-superior">Términos de orden superior</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#explicacion-frente-a-prediccion">Explicación frente a predicción</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explicacion">Explicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion">Predicción</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alejandra Tabares
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>